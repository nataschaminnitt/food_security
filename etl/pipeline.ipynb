{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbff4499",
   "metadata": {},
   "source": [
    "# Forecasting Food Prices in Ethiopia\n",
    "\n",
    "Principal data sources:\n",
    "1. Ethiopia Food Prices Dataset from the World Food Programme Price Database, offering extensive food price data across Ethiopia.\n",
    "2. Ethiopia Food Prices Dataset from the Famine Early Warning Systems Network,\n",
    "\n",
    "Supplementary data sources:\n",
    "1. Ethiopia Food Category Prices from the FAO [https://www.fao.org/worldfoodsituation/foodpricesindex/en/]\n",
    "2. WFP Global Market Monitor, updated weekly [https://data.humdata.org/dataset/global-market-monitor]\n",
    "3. Ethiopia Monthly Rainfall Data from WFP | CHIRPS [https://data.humdata.org/dataset/eth-rainfall-subnational]\n",
    "4. Ethiopia Population Dataset from OCHA [https://data.humdata.org/dataset/cod-ps-eth]\n",
    "5. Conflict in Ethiopia Dataset from the Armed Conflict Location & Event Data Project (ACLED), containing detailed records of conflict events [https://data.humdata.org/dataset/ethiopia-acled-conflict-data]\n",
    "\n",
    "Other datasets to add:\n",
    "1. Ethiopia Crop Production Statistics from Data Kimetrica, providing detailed statistics on crop production []\n",
    "2. US Dollar — Ethiopian Birr Historical Data []\n",
    "3. Anommoly hotspots [https://data.humdata.org/dataset/asap-hotspots-monthly]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2662f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataschajademinnitt/mamba/envs/foodsec/lib/python3.11/site-packages/ckanapi/version.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/Users/nataschajademinnitt/mamba/envs/foodsec/lib/python3.11/site-packages/quantulum3/classifier.py:28: UserWarning: Classifier dependencies not installed. Run pip install quantulum3[classifier] to install them. The classifer helps to dissambiguate units.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "from hdx.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4de52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "REQUEST_TIMEOUT = 90\n",
    "USER_AGENT = {\"User-Agent\": \"Mozilla/5.0 (foodsec-notebook/0.2)\"}\n",
    "\n",
    "# Lookback horizon for price data (approx 5y + 1mo padding for leap years)\n",
    "FIVE_YEARS_AGO_ISO = (date.today() - timedelta(days=5*365 + 30)).isoformat()\n",
    "FIVE_YEARS_AGO_TS  = (pd.Timestamp.today().normalize() - pd.offsets.DateOffset(years=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63882a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Helpers (single source of truth)\n",
    "# ------------------------------------------------------------------------------\n",
    "def norm(s: str, *, lower=True, keep_internal_space=True) -> str:\n",
    "    \"\"\"Trim + collapse spaces; lowercase by default.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s).strip())\n",
    "    return s.lower() if lower else s\n",
    "\n",
    "def month_start(x):\n",
    "    \"\"\"Coerce to month-start Timestamp(s) for scalars, Series, or arrays.\"\"\"\n",
    "    obj = pd.to_datetime(x, errors=\"coerce\")\n",
    "\n",
    "    # Pandas Series → use .dt accessors\n",
    "    if isinstance(obj, pd.Series):\n",
    "        return obj.dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    # Numpy array / list → convert to Series then back\n",
    "    if isinstance(obj, (np.ndarray, list, tuple)):\n",
    "        ser = pd.to_datetime(pd.Series(obj), errors=\"coerce\")\n",
    "        return ser.dt.to_period(\"M\").dt.to_timestamp().to_numpy()\n",
    "\n",
    "    # Scalar (Timestamp/NaT) → use Timestamp methods\n",
    "    if pd.isna(obj):\n",
    "        return pd.NaT\n",
    "    return pd.Timestamp(obj).to_period(\"M\").to_timestamp()\n",
    "\n",
    "\n",
    "def safe_numeric(sr: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(sr, errors=\"coerce\")\n",
    "\n",
    "\n",
    "# --- Product mappings (WFP -> FEWS canonical) ---------------------------------\n",
    "FEWS_CANON = {\n",
    "    \"Maize Grain (White)\", \"Wheat Grain\", \"Goats (Local Quality)\",\n",
    "    \"Sheep (Local Quality)\", \"Mixed Teff\", \"Oxen (Local Quality)\",\n",
    "    \"Rice (Milled)\", \"Horse beans\", \"Refined Vegetable Oil\",\n",
    "    \"Wheat Flour\", \"Sorghum (White)\", \"Refined sugar\", \"Sorghum (Red)\",\n",
    "    \"Beans (Haricot)\", \"Camels (Local Quality)\", \"Diesel\", \"Gasoline\",\n",
    "    \"Firewood\", \"Sorghum (Yellow)\"\n",
    "}\n",
    "\n",
    "DIRECT_MAP = {\n",
    "    \"maize (white)\": \"Maize Grain (White)\",\n",
    "    \"maize\": \"Maize Grain (White)\",\n",
    "    \"wheat flour\": \"Wheat Flour\",\n",
    "    \"wheat (white)\": \"Wheat Grain\",\n",
    "    \"wheat (mixed)\": \"Wheat Grain\",\n",
    "    \"wheat\": \"Wheat Grain\",\n",
    "    \"rice\": \"Rice (Milled)\",\n",
    "    \"sorghum (white)\": \"Sorghum (White)\",\n",
    "    \"sorghum (red)\": \"Sorghum (Red)\",\n",
    "    \"sorghum (yellow)\": \"Sorghum (Yellow)\",\n",
    "    \"oil (vegetable)\": \"Refined Vegetable Oil\",\n",
    "    \"sugar\": \"Refined sugar\",\n",
    "    \"beans (haricot)\": \"Beans (Haricot)\",\n",
    "    \"beans (fava)\": \"Horse beans\",\n",
    "    \"horse beans\": \"Horse beans\",\n",
    "    \"teff (mixed)\": \"Mixed Teff\",\n",
    "    \"teff (white)\": \"Mixed Teff\",\n",
    "    \"teff (red)\": \"Mixed Teff\",\n",
    "    \"teff (sergegna)\": \"Mixed Teff\",\n",
    "    \"livestock (goat)\": \"Goats (Local Quality)\",\n",
    "    \"livestock (sheep)\": \"Sheep (Local Quality)\",\n",
    "    \"livestock (camel)\": \"Camels (Local Quality)\",\n",
    "    \"livestock (ox)\": \"Oxen (Local Quality)\",\n",
    "    \"livestock (bull)\": \"Oxen (Local Quality)\",\n",
    "    \"livestock (cattle)\": \"Oxen (Local Quality)\",\n",
    "    \"diesel\": \"Diesel\",\n",
    "    \"gasoline\": \"Gasoline\",\n",
    "}\n",
    "\n",
    "REGEX_RULES = [\n",
    "    (r\"^maize.*white\", \"Maize Grain (White)\"),\n",
    "    (r\"^wheat.*flour\", \"Wheat Flour\"),\n",
    "    (r\"^wheat\\b\", \"Wheat Grain\"),\n",
    "    (r\"^rice\\b\", \"Rice (Milled)\"),\n",
    "    (r\"^sorghum.*white\", \"Sorghum (White)\"),\n",
    "    (r\"^sorghum.*red\", \"Sorghum (Red)\"),\n",
    "    (r\"^sorghum.*yellow\", \"Sorghum (Yellow)\"),\n",
    "    (r\"^oil.*vegetable\", \"Refined Vegetable Oil\"),\n",
    "    (r\"^beans.*haricot\", \"Beans (Haricot)\"),\n",
    "    (r\"^beans.*fava\", \"Horse beans\"),\n",
    "    (r\"^teff\", \"Mixed Teff\"),\n",
    "    (r\"^livestock.*goat\", \"Goats (Local Quality)\"),\n",
    "    (r\"^livestock.*sheep\", \"Sheep (Local Quality)\"),\n",
    "    (r\"^livestock.*camel\", \"Camels (Local Quality)\"),\n",
    "    (r\"^livestock.*(ox|bull|cattle)\", \"Oxen (Local Quality)\"),\n",
    "    (r\"^diesel$\", \"Diesel\"),\n",
    "    (r\"^gasoline$\", \"Gasoline\"),\n",
    "]\n",
    "\n",
    "def map_wfp_product_to_fews(name: str) -> str | None:\n",
    "    s = norm(name)\n",
    "    for canon in FEWS_CANON:\n",
    "        if norm(canon) == s:\n",
    "            return canon\n",
    "    if s in DIRECT_MAP:\n",
    "        return DIRECT_MAP[s]\n",
    "    for pat, target in REGEX_RULES:\n",
    "        if re.search(pat, s):\n",
    "            return target\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Unit normalization --------------------------------------------------------\n",
    "EXPECTED_UNIT = {\n",
    "    \"Maize Grain (White)\": \"kg\",\n",
    "    \"Wheat Grain\": \"kg\",\n",
    "    \"Wheat Flour\": \"kg\",\n",
    "    \"Rice (Milled)\": \"kg\",\n",
    "    \"Mixed Teff\": \"kg\",\n",
    "    \"Sorghum (White)\": \"kg\",\n",
    "    \"Sorghum (Red)\": \"kg\",\n",
    "    \"Sorghum (Yellow)\": \"kg\",\n",
    "    \"Refined sugar\": \"kg\",\n",
    "    \"Refined Vegetable Oil\": \"l\",\n",
    "    \"Diesel\": \"l\",\n",
    "    \"Gasoline\": \"l\",\n",
    "    \"Goats (Local Quality)\": \"head\",\n",
    "    \"Sheep (Local Quality)\": \"head\",\n",
    "    \"Oxen (Local Quality)\": \"head\",\n",
    "    \"Camels (Local Quality)\": \"head\",\n",
    "}\n",
    "\n",
    "def normalize_unit_and_factor(u: str) -> tuple[str, float]:\n",
    "    s = norm(u).replace(\".\", \"\")\n",
    "    if s in {\"kg\",\"kgs\",\"kilogram\",\"kilograms\"}: return \"kg\", 1.0\n",
    "    if s in {\"l\",\"lt\",\"ltr\",\"litre\",\"liter\",\"litres\",\"liters\"}: return \"l\", 1.0\n",
    "    if s in {\"head\",\"heads\"}: return \"head\", 1.0\n",
    "    if s in {\"ea\",\"each\"}: return \"ea\", 1.0\n",
    "    m = re.match(r\"^(\\d+)\\s*kg$\", s)\n",
    "    if m:\n",
    "        qty = float(m.group(1))\n",
    "        if qty > 0:\n",
    "            return \"kg\", 1.0 / qty\n",
    "    return s, 1.0\n",
    "\n",
    "# --- Admin-1 harmonization (used across datasets) ------------------------------\n",
    "MAP_ADMIN_TO_PANEL = {\n",
    "    # Variants → panel names\n",
    "    \"benishangul-gumuz\": \"B. Gumuz\",\n",
    "    \"benshangul/gumuz\": \"B. Gumuz\",\n",
    "    \"benishangul / gumuz\": \"B. Gumuz\",\n",
    "    \"benishangul gumuz\": \"B. Gumuz\",\n",
    "    \"snnpr\": \"SNNPR\",\n",
    "    \"snnp\": \"SNNPR\",\n",
    "    \"snnp\": \"SNNPR\",\n",
    "    \"south west\": \"South Ethiopia\",\n",
    "    \"sidama\": \"South Ethiopia\",\n",
    "}\n",
    "\n",
    "def harmonize_admin1(name: str) -> str:\n",
    "    n = norm(name)\n",
    "    return MAP_ADMIN_TO_PANEL.get(n, n.title() if n else n)\n",
    "\n",
    "\n",
    "# HDX resolver\n",
    "def _parse_ts(x):\n",
    "    try:\n",
    "        return pd.to_datetime(x, utc=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def resolve_hdx_resource(dataset_slug: str,\n",
    "                         *,\n",
    "                         fmt: str | None = \"xlsx\",\n",
    "                         name_regex: str | None = None):\n",
    "    \"\"\"\n",
    "    Returns (resource_dict, url_string) for the most recently updated resource\n",
    "    in the given HDX dataset that matches fmt and/or name_regex.\n",
    "    \"\"\"\n",
    "    ds = Dataset.read_from_hdx(dataset_slug)\n",
    "    resources = ds.get_resources()\n",
    "    if not resources:\n",
    "        raise RuntimeError(f\"No resources found in HDX dataset: {dataset_slug}\")\n",
    "\n",
    "    # Filter by format and optional name regex\n",
    "    filt = []\n",
    "    for r in resources:\n",
    "        rfmt = str(r.get(\"format\", \"\")).lower()\n",
    "        rname = str(r.get(\"name\", \"\")).strip()\n",
    "        if fmt and rfmt != fmt.lower():\n",
    "            continue\n",
    "        if name_regex and not re.search(name_regex, rname, flags=re.I):\n",
    "            continue\n",
    "        filt.append(r)\n",
    "\n",
    "    cand = filt or resources  # fallback: anything in the dataset\n",
    "    # Sort by last modified-ish fields\n",
    "    cand.sort(key=lambda r: (\n",
    "        _parse_ts(r.get(\"last_modified\") or r.get(\"updated\") or r.get(\"created\"))\n",
    "    ), reverse=True)\n",
    "\n",
    "    res = cand[0]\n",
    "    url = res.get(\"download_url\") or res.get(\"url\")\n",
    "    if not url:\n",
    "        raise RuntimeError(\"HDX resource has no downloadable URL.\")\n",
    "    return res, url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11803694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# FEWS NET prices\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_fewsnet_prices() -> pd.DataFrame:\n",
    "    url = \"https://fdw.fews.net/api/marketpricefacts.csv\"\n",
    "    params = {\n",
    "        \"dataset\": \"FEWS_NET_Staple_Food_Price_Data\",\n",
    "        \"country_code\": \"ET\",\n",
    "        \"start_date\": FIVE_YEARS_AGO_ISO,\n",
    "        \"schedule\": \"Daily\",\n",
    "        \"price_type\": \"Retail\",\n",
    "        \"fields\": \",\".join([\n",
    "            \"period_date\",\"admin_1\",\"market\",\"product\",\"unit_type\",\"unit\",\n",
    "            \"price_type\",\"value\"\n",
    "        ]),\n",
    "        \"format\": \"csv\",\n",
    "    }\n",
    "    r = requests.get(url, params=params, headers=USER_AGENT, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    df = pd.read_csv(\n",
    "        io.StringIO(r.text),\n",
    "        parse_dates=[\"period_date\"],\n",
    "        dtype={\n",
    "            \"admin_1\": \"string\", \"market\": \"string\", \"product\": \"string\",\n",
    "            \"unit_type\": \"string\", \"unit\": \"string\", \"price_type\": \"string\",\n",
    "            \"value\": \"float64\"\n",
    "        },\n",
    "    )\n",
    "    df = df.dropna(subset=[\"period_date\",\"admin_1\",\"market\",\"product\",\"value\"])\n",
    "    return df.assign(source=\"FEWSNET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aabcea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# WFP prices → FEWS-like schema\n",
    "# ------------------------------------------------------------------------------\n",
    "def wfp_to_fewsnet_schema(df_wfp_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_wfp_raw.copy()\n",
    "\n",
    "    # Drop repeated header row if present\n",
    "    if (df.columns == df.iloc[0]).all():\n",
    "        df = df.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "    # Parse & clean\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    for col in [\"price\",\"usdprice\",\"latitude\",\"longitude\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = safe_numeric(df[col])\n",
    "\n",
    "    # Keep recent horizon\n",
    "    df = df[df[\"date\"] >= pd.Timestamp(FIVE_YEARS_AGO_TS)]\n",
    "\n",
    "    # Standardize text columns\n",
    "    for c in [\"admin1\",\"admin2\",\"market\",\"commodity\",\"category\",\"unit\",\"pricetype\",\"currency\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    # Filter to Retail (drop wholesale)\n",
    "    price_type = df[\"pricetype\"].str.casefold().str.strip()\n",
    "    df = df.loc[~price_type.str.contains(\"wholesale\", na=False)].copy()\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"period_date\": df[\"date\"],\n",
    "        \"admin_1\": df[\"admin1\"].replace({\"nan\": np.nan}),\n",
    "        \"market\": df[\"market\"],\n",
    "        \"product\": df[\"commodity\"],\n",
    "        \"unit\": df[\"unit\"],\n",
    "        \"price_type\": \"Retail\",\n",
    "        \"value\": df[\"price\"],\n",
    "    })\n",
    "    out = out.dropna(subset=[\"period_date\",\"admin_1\",\"market\",\"product\",\"value\"])\n",
    "    out[\"value\"] = safe_numeric(out[\"value\"])\n",
    "    out = out[out[\"value\"] > 0]\n",
    "    return out.assign(source=\"WFP\").reset_index(drop=True)\n",
    "\n",
    "def load_wfp_prices() -> pd.DataFrame:\n",
    "    url = (\"https://data.humdata.org/dataset/2e4f1922-e446-4b57-a98a-d0e2d5e34afa/\"\n",
    "           \"resource/87bac18e-f3aa-4b29-8cf8-76763e823dc5/download/wfp_food_prices_eth.csv\")\n",
    "    r = requests.get(url, headers=USER_AGENT, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    df_wfp_raw = pd.read_csv(io.StringIO(r.text))\n",
    "    df = wfp_to_fewsnet_schema(df_wfp_raw)\n",
    "\n",
    "    # Map products to FEWS canonical\n",
    "    df[\"product\"] = df[\"product\"].map(map_wfp_product_to_fews)\n",
    "    df = df.loc[df[\"product\"].isin(FEWS_CANON)].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc07b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Combine, unit normalization, monthly aggregation\n",
    "# ------------------------------------------------------------------------------\n",
    "def combine_and_aggregate(fews: pd.DataFrame, wfp: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"period_date\",\"admin_1\",\"market\",\"product\",\"unit\",\"price_type\",\"value\",\"source\"]\n",
    "    df_all = (pd.concat([fews[cols], wfp[cols]], ignore_index=True)\n",
    "                .drop_duplicates()\n",
    "                .copy())\n",
    "\n",
    "    # (Optional) drop non-food items for modeling\n",
    "    drop_products = [\"Horse beans\",\"Beans (Haricot)\",\"Diesel\",\"Gasoline\",\"Firewood\"]\n",
    "    df_all = df_all.loc[~df_all[\"product\"].isin(drop_products)].copy()\n",
    "\n",
    "    # Normalize units and make monthly key\n",
    "    work = df_all.copy()\n",
    "    work[\"month\"] = work[\"period_date\"].values.astype(\"datetime64[M]\")\n",
    "\n",
    "    unit_parsed = work[\"unit\"].map(normalize_unit_and_factor)\n",
    "    work[\"unit_std\"]    = [u for (u, f) in unit_parsed]\n",
    "    work[\"unit_factor\"] = [f for (u, f) in unit_parsed]\n",
    "    work[\"value_std\"]   = work[\"value\"] * work[\"unit_factor\"]\n",
    "\n",
    "    # Keep only expected units (where we have an expected unit)\n",
    "    exp = work[\"product\"].map(EXPECTED_UNIT)\n",
    "    work = work.loc[exp.isna() | (work[\"unit_std\"] == exp)].copy()\n",
    "\n",
    "    grp = [\"admin_1\",\"month\",\"product\",\"unit_std\",\"source\"]\n",
    "    # First average within source, then across sources (mean/median & counts)\n",
    "    by_src = (work.groupby(grp, as_index=False)\n",
    "                  .agg(value_src_mean=(\"value_std\",\"mean\"),\n",
    "                       value_src_median=(\"value_std\",\"median\"),\n",
    "                       n_obs_src=(\"value_std\",\"count\")))\n",
    "\n",
    "    grp2 = [\"admin_1\",\"month\",\"product\",\"unit_std\"]\n",
    "    out = (by_src.groupby(grp2, as_index=False)\n",
    "                .agg(value_mean=(\"value_src_mean\",\"mean\"),\n",
    "                     value_median=(\"value_src_median\",\"median\"),\n",
    "                     n_obs=(\"n_obs_src\",\"sum\"),\n",
    "                     sources=(\"source\", lambda x: \",\".join(sorted(set(map(str,x)))))))\n",
    "\n",
    "    return out.rename(columns={\"unit_std\":\"unit\"}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e27c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# QC: z-score outlier removal (log-scale optional)\n",
    "# ------------------------------------------------------------------------------\n",
    "def remove_outliers(work: pd.DataFrame, use_log=True, max_z=2.5, max_rz=3.5) -> pd.DataFrame:\n",
    "    dfq = work.copy()\n",
    "    gkeys = [\"admin_1\",\"month\",\"product\",\"unit\"]\n",
    "\n",
    "    vals = np.log(dfq[\"value_median\"]) if use_log else dfq[\"value_median\"]\n",
    "    dfq[\"_val\"] = vals.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    def _mad(x):\n",
    "        med = np.nanmedian(x)\n",
    "        return np.nanmedian(np.abs(x - med))\n",
    "\n",
    "    stats = (dfq.groupby(gkeys, group_keys=False)[\"_val\"]\n",
    "                .agg(n=\"count\",\n",
    "                     mean=\"mean\",\n",
    "                     std=lambda x: np.nanstd(x, ddof=1),\n",
    "                     median=\"median\",\n",
    "                     mad=_mad)\n",
    "                .reset_index())\n",
    "\n",
    "    dfq = dfq.merge(stats, on=gkeys, how=\"left\")\n",
    "\n",
    "    dfq[\"z\"]  = (dfq[\"_val\"] - dfq[\"mean\"]) / dfq[\"std\"]\n",
    "    dfq.loc[(dfq[\"n\"] < 2) | (dfq[\"std\"] <= 0), \"z\"] = np.nan\n",
    "\n",
    "    dfq[\"rz\"] = 0.6745 * (dfq[\"_val\"] - dfq[\"median\"]) / dfq[\"mad\"]\n",
    "    dfq.loc[(dfq[\"n\"] < 2) | (dfq[\"mad\"] <= 0), \"rz\"] = np.nan\n",
    "\n",
    "    clean = dfq[~(dfq[\"z\"].abs() > max_z) & ~(dfq[\"rz\"].abs() > max_rz)].copy()\n",
    "    clean[\"value_for_agg\"] = np.exp(clean[\"_val\"]) if use_log else clean[\"_val\"]\n",
    "\n",
    "    grp = [\"admin_1\",\"month\",\"product\",\"unit\"]\n",
    "    out = (clean.groupby(grp, as_index=False)\n",
    "                .agg(value_mean=(\"value_for_agg\",\"mean\"),\n",
    "                     value_median=(\"value_for_agg\",\"median\"),\n",
    "                     n_obs=(\"value_for_agg\",\"count\")))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9528b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Imputation (temporal + spatial + cross-admin)\n",
    "# ------------------------------------------------------------------------------\n",
    "def impute_prices(df_pm: pd.DataFrame,\n",
    "                  max_interp_gap: int = 2,\n",
    "                  use_spatial: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Requires columns: admin_1, product, unit, month, value_median\n",
    "    Returns: value_imputed, impute_method (+ value_orig)\n",
    "    \"\"\"\n",
    "    req = {\"admin_1\",\"product\",\"unit\",\"month\",\"value_median\"}\n",
    "    missing_cols = req - set(df_pm.columns)\n",
    "    if missing_cols:\n",
    "        raise KeyError(f\"impute_prices missing required columns: {missing_cols}\")\n",
    "\n",
    "    df = df_pm.copy()\n",
    "    df[\"month\"] = pd.to_datetime(df[\"month\"], errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "    gkey = [\"admin_1\",\"product\",\"unit\"]\n",
    "\n",
    "    # complete monthly grid per group\n",
    "    full = []\n",
    "    for (a,p,u), g in df.groupby(gkey, dropna=False):\n",
    "        if g[\"month\"].notna().sum() == 0:\n",
    "            continue\n",
    "        rng = pd.period_range(g[\"month\"].min().to_period(\"M\"), g[\"month\"].max().to_period(\"M\"), freq=\"M\").to_timestamp()\n",
    "        gi = g.set_index(\"month\").reindex(rng)\n",
    "        gi.index.name = \"month\"\n",
    "        gi[\"admin_1\"], gi[\"product\"], gi[\"unit\"] = a, p, u\n",
    "        full.append(gi.reset_index())\n",
    "    if not full:\n",
    "        raise ValueError(\"No valid groups to impute (all months are NaT?)\")\n",
    "\n",
    "    df = pd.concat(full, ignore_index=True)\n",
    "    df[\"value_orig\"]    = df[\"value_median\"]\n",
    "    df[\"value_imputed\"] = df[\"value_median\"]\n",
    "    df[\"impute_method\"] = np.where(df[\"value_median\"].notna(), \"observed\", \"missing\")\n",
    "\n",
    "    # 1) temporal interpolation on log-prices\n",
    "    def interp_group(g):\n",
    "        g = g.sort_values(\"month\")\n",
    "        v = g[\"value_imputed\"]\n",
    "        logv = np.log(v.replace(0, np.nan))\n",
    "\n",
    "        idx_time = pd.DatetimeIndex(pd.to_datetime(g[\"month\"].values), name=\"month\")\n",
    "        s_time = pd.Series(logv.to_numpy(), index=idx_time)\n",
    "        s_interp = s_time.interpolate(method=\"time\", limit=max_interp_gap, limit_direction=\"both\")\n",
    "\n",
    "        filled = v.to_numpy(copy=True)\n",
    "        missing = v.isna().to_numpy()\n",
    "        interp_vals = np.exp(s_interp.to_numpy())\n",
    "        has_interp = ~np.isnan(interp_vals)\n",
    "        use = missing & has_interp\n",
    "\n",
    "        filled[use] = interp_vals[use]\n",
    "        g[\"value_imputed\"] = filled\n",
    "        g.loc[(missing) & (has_interp) & (g[\"impute_method\"]==\"missing\"), \"impute_method\"] = \"interp_time\"\n",
    "        return g\n",
    "\n",
    "    df = (df.sort_values(gkey + [\"month\"])\n",
    "            .groupby(gkey, group_keys=False)\n",
    "            .apply(interp_group)\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # 2) spatial scaled-national fallback\n",
    "    if use_spatial:\n",
    "        nat = (df.groupby([\"product\",\"unit\",\"month\"], as_index=False)\n",
    "                 .agg(national=(\"value_orig\",\"median\")))\n",
    "        merged = df.merge(nat, on=[\"product\",\"unit\",\"month\"], how=\"left\")\n",
    "        merged[\"ratio\"] = merged[\"value_orig\"] / merged[\"national\"]\n",
    "        ratios = (merged.dropna(subset=[\"ratio\"])\n",
    "                        .groupby(gkey, as_index=False)\n",
    "                        .agg(admin_nat_ratio=(\"ratio\",\"median\")))\n",
    "\n",
    "        df = df.merge(ratios, on=gkey, how=\"left\")\n",
    "        df = df.merge(nat.rename(columns={\"national\":\"nat_val\"}), on=[\"product\",\"unit\",\"month\"], how=\"left\")\n",
    "\n",
    "        mask_sp = df[\"value_imputed\"].isna() & df[\"admin_nat_ratio\"].notna() & df[\"nat_val\"].notna()\n",
    "        df.loc[mask_sp, \"value_imputed\"] = df.loc[mask_sp, \"admin_nat_ratio\"] * df.loc[mask_sp, \"nat_val\"]\n",
    "        df.loc[mask_sp & (df[\"impute_method\"]==\"missing\"), \"impute_method\"] = \"scaled_national\"\n",
    "\n",
    "    # 3) cross-admin same-month median\n",
    "    cross = (df.groupby([\"product\",\"unit\",\"month\"], as_index=False)\n",
    "               .agg(cross_admin=(\"value_imputed\",\"median\")))\n",
    "    df = df.merge(cross, on=[\"product\",\"unit\",\"month\"], how=\"left\")\n",
    "    mask_cross = df[\"value_imputed\"].isna() & df[\"cross_admin\"].notna()\n",
    "    df.loc[mask_cross, \"value_imputed\"] = df.loc[mask_cross, \"cross_admin\"]\n",
    "    df.loc[mask_cross & (df[\"impute_method\"]==\"missing\"), \"impute_method\"] = \"cross_admin\"\n",
    "\n",
    "    return df.drop(columns=[\"cross_admin\",\"nat_val\",\"admin_nat_ratio\"], errors=\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebdd9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# FAO Food Price Index + category mapping\n",
    "# ------------------------------------------------------------------------------\n",
    "PRODUCT_TO_FAO = {\n",
    "    # cereals\n",
    "    \"maize grain (white)\": \"cereals\",\n",
    "    \"wheat grain\": \"cereals\",\n",
    "    \"wheat flour\": \"cereals\",\n",
    "    \"mixed teff\": \"cereals\",\n",
    "    \"sorghum (white)\": \"cereals\",\n",
    "    \"sorghum (red)\": \"cereals\",\n",
    "    \"sorghum (yellow)\": \"cereals\",\n",
    "    \"rice (milled)\": \"cereals\",\n",
    "    # meat (livestock)\n",
    "    \"goats (local quality)\": \"meat\",\n",
    "    \"sheep (local quality)\": \"meat\",\n",
    "    \"oxen (local quality)\": \"meat\",\n",
    "    \"camels (local quality)\": \"meat\",\n",
    "    # oils\n",
    "    \"refined vegetable oil\": \"oils\",\n",
    "    # sugar\n",
    "    \"refined sugar\": \"sugar\",\n",
    "}\n",
    "\n",
    "def load_fao_indices() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    url = (\"https://www.fao.org/media/docs/worldfoodsituationlibraries/\"\n",
    "           \"default-document-library/food_price_indices_data_oct25.csv?sfvrsn=523ebd2a_54&download=true\")\n",
    "    r = requests.get(url, headers=USER_AGENT, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    df = pd.read_csv(io.StringIO(r.text), header=2)\n",
    "    df = df.iloc[:, :7].dropna(how=\"all\").reset_index(drop=True)\n",
    "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
    "\n",
    "    # Parse dates\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m\", errors=\"coerce\")\n",
    "    df[\"month\"] = month_start(df[\"Date\"])\n",
    "\n",
    "    rename_map = {\n",
    "        \"Food Price Index\": \"fao_food_price_index\",\n",
    "        \"Meat\": \"meat\",\n",
    "        \"Dairy\": \"dairy\",\n",
    "        \"Cereals\": \"cereals\",\n",
    "        \"Oils\": \"oils\",\n",
    "        \"Sugar\": \"sugar\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    value_cols = [\"meat\",\"dairy\",\"cereals\",\"oils\",\"sugar\"]\n",
    "    long = (df[[\"month\",\"fao_food_price_index\"] + value_cols]\n",
    "              .melt(id_vars=[\"month\",\"fao_food_price_index\"],\n",
    "                    value_vars=value_cols,\n",
    "                    var_name=\"fao_category\",\n",
    "                    value_name=\"fao_category_index\"))\n",
    "    # one-row per month overall FPI (for fallback)\n",
    "    fpi_month = df[[\"month\",\"fao_food_price_index\"]].drop_duplicates(\"month\")\n",
    "    return long, fpi_month\n",
    "\n",
    "def merge_fao(df_prices: pd.DataFrame, fao_long: pd.DataFrame, fpi_month: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_prices.copy()\n",
    "    df[\"fao_category\"] = df[\"product\"].map(lambda x: PRODUCT_TO_FAO.get(norm(x), \"other\"))\n",
    "    out = df.merge(fao_long, on=[\"month\",\"fao_category\"], how=\"left\", validate=\"many_to_one\")\n",
    "    out = out.merge(fpi_month, on=\"month\", how=\"left\", suffixes=(\"\",\"_overall\"))\n",
    "    out[\"fao_food_price_index\"] = out[\"fao_food_price_index\"].fillna(out[\"fao_food_price_index_overall\"])\n",
    "    return out.drop(columns=[\"fao_food_price_index_overall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "409df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# ACLED monthly events/fatalities by admin-1\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_acled_monthly() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch the latest ACLED monthly admin1 events/fatalities for Ethiopia from HDX,\n",
    "    robust to changing filenames/signed URLs.\n",
    "    \"\"\"\n",
    "    # Pick the dataset by its HDX slug (stable), not a dated file URL\n",
    "    DATASET_SLUG = \"ethiopia-acled-conflict-data\"  # HDX dataset key\n",
    "\n",
    "    # Look for an Excel resource that sounds like monthly violence/events\n",
    "    # (regex is forgiving; adjust if HDX renames things)\n",
    "    NAME_RE = r\"(political|violence|conflict).*month|monthly|by\\s*month\"\n",
    "\n",
    "    try:\n",
    "        res, url = resolve_hdx_resource(DATASET_SLUG, fmt=\"xlsx\", name_regex=NAME_RE)\n",
    "        r = requests.get(url, headers=USER_AGENT, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        # If that specific match fails, try the latest Excel resource regardless of name.\n",
    "        try:\n",
    "            res, url = resolve_hdx_resource(DATASET_SLUG, fmt=\"xlsx\", name_regex=None)\n",
    "            r = requests.get(url, headers=USER_AGENT, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            r.raise_for_status()\n",
    "        except Exception as e2:\n",
    "            # Last-resort: return an empty frame so the pipeline can still run\n",
    "            print(f\"[WARN] Couldn’t fetch ACLED from HDX ({e2}). Returning empty monthly table.\")\n",
    "            return pd.DataFrame(columns=[\"admin_1\",\"month\",\"Events\",\"Fatalities\"])\n",
    "\n",
    "    # Load the workbook; prefer a sheet literally named “Data” else first sheet\n",
    "    try:\n",
    "        xls = pd.ExcelFile(io.BytesIO(r.content))\n",
    "        sheet = \"Data\" if \"Data\" in xls.sheet_names else xls.sheet_names[0]\n",
    "        df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed reading ACLED Excel: {e}\")\n",
    "\n",
    "    # Try to detect columns (different files sometimes rename headers)\n",
    "    colmap = {}\n",
    "    # admin column\n",
    "    for c in df.columns:\n",
    "        if re.fullmatch(r\"(?i)admin\\s*1|admin1|region|state\", str(c).strip(), flags=0):\n",
    "            colmap[\"Admin1\"] = c; break\n",
    "    # month column\n",
    "    for c in df.columns:\n",
    "        if re.fullmatch(r\"(?i)month\", str(c).strip()):\n",
    "            colmap[\"Month\"] = c; break\n",
    "    # year column\n",
    "    for c in df.columns:\n",
    "        if re.fullmatch(r\"(?i)year\", str(c).strip()):\n",
    "            colmap[\"Year\"] = c; break\n",
    "    # events / fatalities\n",
    "    for c in df.columns:\n",
    "        if re.fullmatch(r\"(?i)events?\", str(c).strip()):\n",
    "            colmap[\"Events\"] = c; break\n",
    "    for c in df.columns:\n",
    "        if re.fullmatch(r\"(?i)fatalit(y|ies)\", str(c).strip()):\n",
    "            colmap[\"Fatalities\"] = c; break\n",
    "\n",
    "    required = {\"Admin1\",\"Month\",\"Year\",\"Events\",\"Fatalities\"}\n",
    "    if not required.issubset(colmap):\n",
    "        # If any are missing, try to infer a pre-aggregated monthly sheet\n",
    "        # that already has a full date column like 'month' or 'date'\n",
    "        alt_time_col = None\n",
    "        for c in df.columns:\n",
    "            if re.fullmatch(r\"(?i)(month|date)\", str(c).strip()):\n",
    "                alt_time_col = c; break\n",
    "        if alt_time_col and \"Admin1\" in colmap and \"Events\" in colmap and \"Fatalities\" in colmap:\n",
    "            df[\"month\"] = month_start(df[alt_time_col])\n",
    "            df[\"admin_1\"] = df[colmap[\"Admin1\"]].map(harmonize_admin1)\n",
    "            out = (df.dropna(subset=[\"admin_1\",\"month\"])\n",
    "                     .groupby([\"admin_1\",\"month\"], as_index=False)\n",
    "                     .agg(Events=(\"Events\",\"sum\"), Fatalities=(\"Fatalities\",\"sum\")))\n",
    "            return out\n",
    "        raise KeyError(f\"Expected columns not found in ACLED file. Found: {list(df.columns)}\")\n",
    "\n",
    "    # Standard path: combine year + month to a Timestamp\n",
    "    df = df.rename(columns={colmap[k]: k for k in colmap})\n",
    "    df[\"Month\"] = df[\"Month\"].astype(str).str.strip()\n",
    "    m1 = pd.to_datetime(df[\"Year\"].astype(int).astype(str) + \"-\" + df[\"Month\"], format=\"%Y-%B\", errors=\"coerce\")\n",
    "    m2 = pd.to_datetime(df[\"Year\"].astype(int).astype(str) + \"-\" + df[\"Month\"], format=\"%Y-%b\", errors=\"coerce\")\n",
    "    m3 = pd.to_datetime(df[\"Year\"].astype(int).astype(str) + \"-\" + df[\"Month\"].str.zfill(2), format=\"%Y-%m\", errors=\"coerce\")\n",
    "    df[\"month\"] = m1.fillna(m2).fillna(m3)\n",
    "    df[\"month\"] = month_start(df[\"month\"])\n",
    "\n",
    "    df[\"admin_1\"] = df[\"Admin1\"].map(harmonize_admin1)\n",
    "    out = (df.drop(columns=[\"Month\",\"Year\"])\n",
    "             .dropna(subset=[\"admin_1\",\"month\"])\n",
    "             .groupby([\"admin_1\",\"month\"], as_index=False)\n",
    "             .agg(Events=(\"Events\",\"sum\"), Fatalities=(\"Fatalities\",\"sum\")))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9dc1a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# CHIRPS rainfall (subnational monthly)\n",
    "# ------------------------------------------------------------------------------\n",
    "PCODE1_TO_NAME = {\n",
    "    \"ET01\":\"Afar\",\"ET02\":\"Amhara\",\"ET03\":\"Benishangul-Gumuz\",\"ET04\":\"Dire Dawa\",\n",
    "    \"ET05\":\"Gambela\",\"ET06\":\"Harari\",\"ET07\":\"Oromia\",\"ET08\":\"Somali\",\n",
    "    \"ET09\":\"SNNPR\",\"ET10\":\"Tigray\",\"ET11\":\"Addis Ababa\",\n",
    "    \"ET12\":\"Sidama\",\"ET13\":\"South West\",\"ET14\":\"Central Ethiopia\",\"ET15\":\"South Ethiopia\",\n",
    "}\n",
    "\n",
    "def load_rainfall_monthly() -> pd.DataFrame:\n",
    "    url = (\"https://data.humdata.org/dataset/423143be-315f-48d7-9e90-ae23738da564/\"\n",
    "           \"resource/49e3a707-d153-423e-b22b-30484d678dd7/download/eth-rainfall-subnat-full.csv\")\n",
    "    r = requests.get(url, headers=USER_AGENT, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    rf = pd.read_csv(io.StringIO(r.text))\n",
    "\n",
    "    rf.columns = [c.strip() for c in rf.columns]\n",
    "    rf[\"date\"]  = pd.to_datetime(rf[\"date\"], errors=\"coerce\")\n",
    "    rf[\"PCODE\"] = rf[\"PCODE\"].astype(str).str.strip().str.upper()\n",
    "    rf = rf.dropna(subset=[\"date\",\"PCODE\"]).copy()\n",
    "\n",
    "    rf[\"admin1_pcode\"]   = rf[\"PCODE\"].str.extract(r\"^(ET\\d{2})\")\n",
    "    rf[\"admin1_name_raw\"] = rf[\"admin1_pcode\"].map(PCODE1_TO_NAME)\n",
    "    rf[\"admin_1\"]        = rf[\"admin1_name_raw\"].map(harmonize_admin1)\n",
    "\n",
    "    num_cols = [c for c in [\"n_pixels\",\"rfh\",\"rfh_avg\",\"r1h\",\"r1h_avg\",\"r3h\",\"r3h_avg\",\"rfq\",\"r1q\",\"r3q\"] if c in rf.columns]\n",
    "    for c in num_cols:\n",
    "        rf[c] = (rf[c].astype(str)\n",
    "                    .str.replace(\",\", \"\", regex=False)\n",
    "                    .str.replace(\"%\", \"\", regex=False)\n",
    "                    .str.replace(\"\\u2014\", \"\", regex=False)  # em dash\n",
    "                    .str.replace(\"-\", \"\", regex=False)\n",
    "                    .str.strip())\n",
    "        rf[c] = safe_numeric(rf[c])\n",
    "\n",
    "    rf[\"month\"] = month_start(rf[\"date\"])\n",
    "    rain = (rf.groupby([\"admin_1\",\"month\"], as_index=False)\n",
    "              .agg(rfh_month=(\"rfh\",\"sum\"),\n",
    "                   rfh_avg_month=(\"rfh_avg\",\"sum\"),\n",
    "                   rfq_month=(\"rfq\",\"mean\")))\n",
    "    rain[\"rain_anom_pct\"] = np.where(\n",
    "        rain[\"rfh_avg_month\"].gt(0),\n",
    "        100.0 * (rain[\"rfh_month\"]/rain[\"rfh_avg_month\"] - 1.0),\n",
    "        np.nan\n",
    "    )\n",
    "    return rain\n",
    "\n",
    "def merge_rainfall(df_panel: pd.DataFrame, rain_m: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_panel.copy()\n",
    "    df[\"admin_1\"] = df[\"admin_1\"].map(harmonize_admin1)\n",
    "    df[\"month\"]   = month_start(df[\"month\"])\n",
    "\n",
    "    # Clip to rainfall coverage to avoid early NaNs (optional)\n",
    "    min_r, max_r = rain_m[\"month\"].min(), rain_m[\"month\"].max()\n",
    "    df = df[(df[\"month\"] >= min_r) & (df[\"month\"] <= max_r)].copy()\n",
    "\n",
    "    return df.merge(\n",
    "        rain_m[[\"admin_1\",\"month\",\"rfh_month\",\"rfh_avg_month\",\"rfq_month\",\"rain_anom_pct\"]],\n",
    "        on=[\"admin_1\",\"month\"], how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39fb4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# WFP Global Market Monitor (signal-only features)\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_gmm_features() -> pd.DataFrame:\n",
    "    url = (\"https://data.humdata.org/dataset/67259d7e-1554-4ffd-be8d-97244577546a/\"\n",
    "           \"resource/2caea41d-2079-44a5-a52b-8bb62a11010f/download/global-market-monitor_subnational.csv\")\n",
    "    r = requests.get(url, headers=USER_AGENT, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    gmm = pd.read_csv(io.StringIO(r.text))\n",
    "\n",
    "    gmm = gmm[\n",
    "        (gmm[\"CountryName\"]==\"Ethiopia\") &\n",
    "        (gmm[\"DataLevel\"].str.contains(\"Subnational\", case=False, na=False))\n",
    "    ].copy()\n",
    "    gmm = gmm[gmm[\"PriceType\"].str.contains(\"Retail\", case=False, na=False)].copy()\n",
    "\n",
    "    gmm[\"Date\"]  = pd.to_datetime(gmm[\"Date\"], errors=\"coerce\")\n",
    "    gmm[\"month\"] = month_start(gmm[\"Date\"])\n",
    "    gmm[\"admin_1\"] = gmm[\"Admin1\"].map(harmonize_admin1)\n",
    "\n",
    "    ptm = (gmm[\"PriceTrendMonth\"].astype(str)\n",
    "           .str.strip()\n",
    "           .str.replace(r\"^nan$\", \"N/A\", case=False, regex=True)\n",
    "           .str.title())\n",
    "    sev_map = {\"Negative\": -1, \"Normal\": 0, \"Moderate\": 1, \"High\": 2, \"Severe\": 3}\n",
    "    gmm[\"ptm_severity\"] = ptm.map(sev_map)\n",
    "\n",
    "    return gmm[[\"admin_1\",\"month\",\"ptm_severity\"]].drop_duplicates()\n",
    "\n",
    "def merge_gmm(df_panel: pd.DataFrame, gmm_feat: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_panel.copy()\n",
    "    df[\"admin_1\"] = df[\"admin_1\"].map(harmonize_admin1)\n",
    "    df[\"month\"]   = month_start(df[\"month\"])\n",
    "    return df.merge(gmm_feat, on=[\"admin_1\",\"month\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "872fa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# OCHA Population (admin1, 2023)\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_population_admin1() -> pd.DataFrame:\n",
    "    url = (\"https://data.humdata.org/dataset/3d9b037f-5112-4afd-92a7-190a9082bd80/\"\n",
    "           \"resource/f82b20f1-8a76-46e9-ba9a-29e531f7af3c/download/eth_admpop_2023.xlsx\")\n",
    "    r = requests.get(url, headers=USER_AGENT, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    pop_raw = pd.read_excel(io.BytesIO(r.content), sheet_name=\"ETH_admpop_adm1_2023\")\n",
    "    pop = pop_raw.copy()\n",
    "    pop.columns = [re.sub(r\"\\s+\",\" \", str(c).strip()) for c in pop.columns]\n",
    "\n",
    "    name_col  = next((c for c in pop.columns if re.fullmatch(r\"(?i)admin1name_en\", c)), None)\n",
    "    pcode_col = next((c for c in pop.columns if re.fullmatch(r\"(?i)admin1pcode\", c)), None)\n",
    "\n",
    "    cand_pop = [c for c in pop.columns if re.search(r\"(?i)(population|total)\", c)]\n",
    "    pop_col = None\n",
    "    for c in cand_pop:\n",
    "        if safe_numeric(pop[c]).notna().sum() > 0:\n",
    "            pop_col = c\n",
    "            break\n",
    "    if pop_col is None:\n",
    "        raise ValueError(\"Population total column not found.\")\n",
    "\n",
    "    out = (pop[[name_col, pcode_col, pop_col]]\n",
    "           .rename(columns={name_col:\"admin1Name_en\", pcode_col:\"admin1Pcode\", pop_col:\"population_2023\"})\n",
    "           .dropna(subset=[\"admin1Name_en\"]))\n",
    "    out[\"population_2023\"] = (out[\"population_2023\"].astype(str)\n",
    "                              .str.replace(\",\", \"\", regex=False)\n",
    "                              .str.replace(\"\\u202f\", \"\", regex=False)\n",
    "                              .str.strip())\n",
    "    out[\"population_2023\"] = safe_numeric(out[\"population_2023\"])\n",
    "    out[\"admin_1\"] = out[\"admin1Name_en\"].map(harmonize_admin1)\n",
    "    return out[[\"admin_1\",\"population_2023\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63121698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/1510956910.py:48: DtypeWarning: Columns (4,5,6,9,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_wfp_raw = pd.read_csv(io.StringIO(r.text))\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/1510956910.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
      "/Users/nataschajademinnitt/mamba/envs/foodsec/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:2015: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/2154112524.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(interp_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Couldn’t fetch ACLED from HDX (There is no HDX configuration! Use Configuration.create(**kwargs)). Returning empty monthly table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/3022802937.py:34: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  out[\"Events\"]     = out[\"Events\"].fillna(0).astype(int)\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/3022802937.py:35: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  out[\"Fatalities\"] = out[\"Fatalities\"].fillna(0).astype(int)\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/2718766468.py:16: DtypeWarning: Columns (1,2,4,5,6,7,8,9,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rf = pd.read_csv(io.StringIO(r.text))\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/2718766468.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rf[\"date\"]  = pd.to_datetime(rf[\"date\"], errors=\"coerce\")\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_63293/2135021595.py:9: DtypeWarning: Columns (1,3,9,10,11,12,13,14,15,18,19,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gmm = pd.read_csv(io.StringIO(r.text))\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# MAIN PIPELINE\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) Load & combine prices\n",
    "df_fews = load_fewsnet_prices()\n",
    "df_wfp  = load_wfp_prices()\n",
    "df_monthly = combine_and_aggregate(df_fews, df_wfp)\n",
    "\n",
    "# 2) QC outliers (log-scale)\n",
    "df_monthly_qc = remove_outliers(df_monthly, use_log=True, max_z=2.5, max_rz=3.5)\n",
    "\n",
    "# 3) Impute gaps\n",
    "df_imp = impute_prices(df_monthly_qc.rename(columns={\"value_median\":\"value_median\"}), max_interp_gap=2, use_spatial=True)\n",
    "# Optional quick check:\n",
    "# df_imp[\"impute_method\"].value_counts(dropna=False)\n",
    "\n",
    "# 4) FAO indices\n",
    "fao_long, fpi_month = load_fao_indices()\n",
    "df_with_fao = merge_fao(df_imp, fao_long, fpi_month)\n",
    "\n",
    "# 5) ACLED\n",
    "acled_m = load_acled_monthly()\n",
    "df_with_conflict = merge_acled(df_with_fao, acled_m)\n",
    "\n",
    "# 6) Rainfall\n",
    "rain_m = load_rainfall_monthly()\n",
    "df_with_rain = merge_rainfall(df_with_conflict, rain_m)\n",
    "\n",
    "# 7) Global Market Monitor signals\n",
    "gmm_feat = load_gmm_features()\n",
    "df_with_gmm = merge_gmm(df_with_rain, gmm_feat)\n",
    "\n",
    "# 8) Population (admin1 static)\n",
    "pop_admin1 = load_population_admin1()\n",
    "df_final = df_with_gmm.merge(pop_admin1, on=\"admin_1\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f267f3a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data/processed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SAVE TIDY OUTPUTS\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/processed/ethiopia_foodprices_model_panel.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/foodsec/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/foodsec/lib/python3.11/site-packages/pandas/core/frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/foodsec/lib/python3.11/site-packages/pandas/io/parquet.py:482\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m impl = get_engine(engine)\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/foodsec/lib/python3.11/site-packages/pandas/io/parquet.py:199\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m     merged_metadata = {**existing_metadata, **df_metadata}\n\u001b[32m    197\u001b[39m     table = table.replace_schema_metadata(merged_metadata)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    207\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io.BufferedWriter)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    209\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[32m    210\u001b[39m ):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/foodsec/lib/python3.11/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/foodsec/lib/python3.11/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/foodsec/lib/python3.11/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'data/processed'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# SAVE TIDY OUTPUTS\n",
    "# ------------------------------------------------------------------------------\n",
    "df_final.to_parquet(\"data/processed/ethiopia_foodprices_model_panel.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3005d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodsec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
