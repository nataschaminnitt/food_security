{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcd5e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nataschajademinnitt/mamba/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.callback import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d239e7d",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "**Lag Features** \n",
    "Prices today often depend on prices in the past — the model can’t infer that automatically from timestamps, so we explicitly provide recent values.\n",
    "\n",
    "* lag 1 = 1 month\n",
    "* lag 3 = 3 months\n",
    "* lag 6 = 6 months\n",
    "\n",
    "**Rolling statistics**\n",
    "Beyond single past points, we want the model to know recent trend and volatility.\n",
    "* roll_mean_3 = mean of previous 3 months\n",
    "* roll_std_3 = standard deviation of previous 3 months\n",
    "* roll_mean_6 = mean of previous 6 months\n",
    "\n",
    "**Seasonal encoding**\n",
    "This gives two coordinates on the unit circle — January and December are now close again. Models can then learn patterns like “prices rise around harvest season” continuously across years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a7f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(\"/Users/nataschajademinnitt/Documents/5_data/food_security/etl/ethiopia_foodprices_model_panel.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2dca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# BUILD FORECASTING FEATURES\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def add_missing_flags(df, cols):\n",
    "    for c in cols:\n",
    "        df[f\"{c}_was_missing\"] = df[c].isna().astype(\"int8\")\n",
    "    return df\n",
    "\n",
    "def national_same_month_fill(df, col):\n",
    "    # median over all regions/products at the same month\n",
    "    same_mo = df.groupby(\"month\")[col].transform(\"median\")\n",
    "    df[col] = df[col].fillna(same_mo)\n",
    "    return df\n",
    "\n",
    "def past_only_roll_fill(df, key_cols, col, window=3):\n",
    "    # per group, use past-only rolling mean as a gentle fallback\n",
    "    df = df.sort_values(key_cols + [\"month\"])\n",
    "    def _roll(g):\n",
    "        s = g[col]\n",
    "        s_fallback = (\n",
    "            s.shift(1)  # ensure past-only\n",
    "             .rolling(window=window, min_periods=1)\n",
    "             .mean()\n",
    "        )\n",
    "        return s.fillna(s_fallback)\n",
    "    df[col] = df.groupby(key_cols, group_keys=False)[col].apply(_roll)\n",
    "    return df\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols: list[str], dtype=float) -> pd.DataFrame:\n",
    "    \"\"\"Add any missing columns with NaN so downstream code is safe.\"\"\"\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "        if dtype is not None and c in df.columns:\n",
    "            try:\n",
    "                df[c] = df[c].astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def month_start_series(s):\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "\n",
    "def impute_features_for_forecasting(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    if \"month\" in df.columns:\n",
    "        df[\"month\"] = month_start_series(df[\"month\"])\n",
    "\n",
    "    # --- Ensure optional columns exist so code never KeyErrors\n",
    "    rain_cols = [\"rfh_month\", \"rfh_avg_month\", \"rfq_month\", \"rain_anom_pct\"]\n",
    "    df = ensure_cols(df, rain_cols, dtype=\"float64\")\n",
    "\n",
    "    fao_cols = [\"fao_category_index\", \"fao_food_price_index\"]\n",
    "    df = ensure_cols(df, fao_cols, dtype=\"float64\")\n",
    "\n",
    "    if \"ptm_severity\" not in df.columns:\n",
    "        df[\"ptm_severity\"] = np.nan\n",
    "    if \"population_2023\" not in df.columns:\n",
    "        df[\"population_2023\"] = np.nan\n",
    "\n",
    "    # --- Missingness flags BEFORE filling\n",
    "    for c in rain_cols:\n",
    "        df[f\"{c}_was_missing\"] = df[c].isna().astype(\"int8\")\n",
    "\n",
    "    # --- Rain: recompute anomaly where possible\n",
    "    can_compute = df[\"rfh_month\"].notna() & df[\"rfh_avg_month\"].gt(0)\n",
    "    df.loc[can_compute, \"rain_anom_pct\"] = 100.0 * (\n",
    "        df.loc[can_compute, \"rfh_month\"] / df.loc[can_compute, \"rfh_avg_month\"] - 1.0\n",
    "    )\n",
    "\n",
    "    # Same-month “national” median (over all rows in that month)\n",
    "    def fill_same_month(col):\n",
    "        df[col] = df[col].fillna(df.groupby(\"month\")[col].transform(\"median\"))\n",
    "    for c in [\"rfh_month\", \"rfh_avg_month\", \"rfq_month\", \"rain_anom_pct\"]:\n",
    "        if c in df.columns:\n",
    "            fill_same_month(c)\n",
    "\n",
    "    # Past-only rolling fallback per admin_1\n",
    "    def past_roll(col, window=3):\n",
    "        df.sort_values([\"admin_1\", \"month\"], inplace=True)\n",
    "        df[col] = (df.groupby(\"admin_1\", group_keys=False)[col]\n",
    "                     .apply(lambda s: s.fillna(s.shift(1).rolling(window, min_periods=1).mean())))\n",
    "    for c in [\"rfh_month\", \"rfh_avg_month\", \"rfq_month\", \"rain_anom_pct\"]:\n",
    "        if c in df.columns:\n",
    "            past_roll(c, window=3)\n",
    "\n",
    "    # --- GMM: ptm_severity (ordinal with many gaps) + flag\n",
    "    df[\"ptm_missing\"] = df[\"ptm_severity\"].isna().astype(\"int8\")\n",
    "    df[\"ptm_severity\"] = df[\"ptm_severity\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "    # --- Population: static per admin\n",
    "    df[\"pop_missing\"] = df[\"population_2023\"].isna().astype(\"int8\")\n",
    "    admin_med = df.groupby(\"admin_1\")[\"population_2023\"].transform(\"median\")\n",
    "    df[\"population_2023\"] = df[\"population_2023\"].fillna(admin_med)\n",
    "    df[\"population_2023\"] = df[\"population_2023\"].fillna(df[\"population_2023\"].median())\n",
    "\n",
    "    # --- FAO indices: same-month fill, then global ffill by time\n",
    "    for c in fao_cols:\n",
    "        df[c] = df[c].fillna(df.groupby(\"month\")[c].transform(\"median\"))\n",
    "    df.sort_values([\"month\"], inplace=True)\n",
    "    for c in fao_cols:\n",
    "        df[c] = df[c].ffill()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_forecast_matrix(df):\n",
    "    \"\"\"\n",
    "    df: your tidy panel with value_imputed and exogenous features (after imputation).\n",
    "    Returns df_model with lags/rolling stats & calendar features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([\"admin_1\",\"product\",\"month\"])\n",
    "\n",
    "    # Lags & rolling (past-only)\n",
    "    grp = df.groupby([\"admin_1\",\"product\"], group_keys=False)\n",
    "    df[\"y\"] = df[\"value_imputed\"]\n",
    "    df[\"y_lag1\"] = grp[\"y\"].shift(1)\n",
    "    df[\"y_lag3\"] = grp[\"y\"].shift(3)\n",
    "    df[\"y_lag6\"] = grp[\"y\"].shift(6)\n",
    "\n",
    "    df[\"roll_mean_3\"] = grp[\"y\"].apply(lambda s: s.shift(1).rolling(3, min_periods=1).mean())\n",
    "    df[\"roll_std_3\"]  = grp[\"y\"].apply(lambda s: s.shift(1).rolling(3, min_periods=2).std())\n",
    "    df[\"roll_mean_6\"] = grp[\"y\"].apply(lambda s: s.shift(1).rolling(6, min_periods=2).mean())\n",
    "\n",
    "    # Calendar features\n",
    "    df[\"month_num\"] = df[\"month\"].dt.month\n",
    "    # Optional seasonal encoding\n",
    "    df[\"mo_sin\"] = np.sin(2*np.pi*df[\"month_num\"]/12)\n",
    "    df[\"mo_cos\"] = np.cos(2*np.pi*df[\"month_num\"]/12)\n",
    "\n",
    "    # Drop rows that don’t have minimal lags (optional, e.g., first 6 months per series)\n",
    "    df = df[df[\"y_lag1\"].notna()].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = impute_features_for_forecasting(df)\n",
    "df_model = build_forecast_matrix(df_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e81e9",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdce9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Helpers =========\n",
    "def smape(y_true, y_pred, eps=1e-9):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return np.mean(np.abs(y_pred - y_true) / denom) * 100.0\n",
    "\n",
    "def pick_features(df: pd.DataFrame):\n",
    "    # Columns to exclude from X (target, keys, and QC-only fields)\n",
    "    drop = {\n",
    "        \"y\", \"value_imputed\", \"value_median\", \"value_mean\", \"value_orig\",\n",
    "        \"n_obs\", \"impute_method\", \"sources\",\n",
    "        \"month\", \"period_date\",  # keys / time\n",
    "    }\n",
    "    # Add any columns that are not numeric (we’ll encode 2 categoricals below)\n",
    "    non_num = set(df.columns[df.dtypes == \"object\"])\n",
    "    # We'll keep admin_1 & product after encoding; drop other object cols if present\n",
    "    drop |= (non_num - {\"admin_1\", \"product\"})\n",
    "    feats = [c for c in df.columns if c not in drop]\n",
    "    return feats\n",
    "\n",
    "def encode_categoricals(df: pd.DataFrame):\n",
    "    \"\"\"Ordinal-encode admin_1 and product to avoid huge one-hot matrices.\n",
    "    For many categories, this is compact & fast; models like XGB handle it well.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for col in [\"admin_1\", \"product\"]:\n",
    "        if col in out.columns:\n",
    "            out[col] = out[col].astype(\"category\")\n",
    "            out[f\"{col}_code\"] = out[col].cat.codes.astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, test_horizon_months: int = 6):\n",
    "    \"\"\"Hold out the last H months as test set (by calendar), keeping all series.\"\"\"\n",
    "    last_month = df[\"month\"].max().to_period(\"M\").to_timestamp()\n",
    "    cut_month  = (last_month.to_period(\"M\") - test_horizon_months).to_timestamp()\n",
    "    train = df[df[\"month\"] <= cut_month].copy()\n",
    "    test  = df[df[\"month\"] >  cut_month].copy()\n",
    "    return train, test\n",
    "\n",
    "def get_X_y(df: pd.DataFrame, feature_cols: list[str]):\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"y\"].astype(\"float32\").to_numpy()\n",
    "    return X, y\n",
    "\n",
    "def eval_report(y_true, y_pred, label=\"\"):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    s    = smape(y_true, y_pred)\n",
    "    print(f\"{label} | MAE: {mae:,.3f}  RMSE: {rmse:,.3f}  sMAPE: {s:,.2f}%\")\n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"smape\": s}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgb_compat(params, X_tr, y_tr, X_val=None, y_val=None, early_rounds=100):\n",
    "    \"\"\"\n",
    "    Tries XGBoost >=2.0 API first (callbacks+eval_metric in params).\n",
    "    Falls back to <2.0 API (eval_metric + early_stopping_rounds in fit).\n",
    "    Last fallback: no early stopping.\n",
    "    \"\"\"\n",
    "    # 1) Try v2-style: callbacks + eval_metric in params\n",
    "    try:\n",
    "        from xgboost.callback import EarlyStopping  # only present in newer versions\n",
    "        model = XGBRegressor(**params)\n",
    "        if X_val is not None:\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[EarlyStopping(rounds=early_rounds, save_best=True)]\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        return model\n",
    "    except TypeError:\n",
    "        pass  # fall through to older API\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # 2) Try v1-style: eval_metric + early_stopping_rounds in fit\n",
    "    try:\n",
    "        model = XGBRegressor(**{k:v for k,v in params.items() if k != \"eval_metric\"})\n",
    "        if X_val is not None:\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=params.get(\"eval_metric\", \"rmse\"),\n",
    "                early_stopping_rounds=early_rounds,\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        return model\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # 3) Last resort: just fit without early stopping\n",
    "    model = XGBRegressor(**{k:v for k,v in params.items() if k != \"eval_metric\"})\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20f2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Prepare data =========\n",
    "# df_model should already exist: keys: admin_1, product, month; target: y\n",
    "assert {\"admin_1\", \"product\", \"month\", \"y\"}.issubset(df_model.columns)\n",
    "\n",
    "# Encode categoricals and keep the encoded columns as features\n",
    "dfX = encode_categoricals(df_model)\n",
    "feature_cols = pick_features(dfX)\n",
    "\n",
    "# Make sure encoded cols are included\n",
    "for col in [\"admin_1_code\", \"product_code\"]:\n",
    "    if col in dfX.columns and col not in feature_cols:\n",
    "        feature_cols.append(col)\n",
    "\n",
    "# Optional: ensure all features are numeric\n",
    "dfX[feature_cols] = dfX[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# ========= Train/Test split (forecast holdout) =========\n",
    "HORIZON = 6  # months\n",
    "train_df, test_df = time_based_split(dfX, test_horizon_months=HORIZON)\n",
    "X_train, y_train = get_X_y(train_df, feature_cols)\n",
    "X_test,  y_test  = get_X_y(test_df,  feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc7d46",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e635678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c93339",
   "metadata": {},
   "source": [
    "## Optimized – Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cdc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 10:27:56,450] A new study created in memory with name: xgb-forecast-smape\n",
      "[I 2025-10-25 10:28:02,203] Trial 0 finished with value: 38.96247100830078 and parameters: {'n_estimators': 1714, 'learning_rate': 0.014980442194867364, 'max_depth': 3, 'min_child_weight': 0.04118537632553031, 'subsample': 0.5296816210385351, 'colsample_bytree': 0.7863047583700602, 'gamma': 2.340408897143101, 'reg_alpha': 1.4877487247850677e-07, 'reg_lambda': 0.00039287435670747837}. Best is trial 0 with value: 38.96247100830078.\n",
      "[I 2025-10-25 10:28:10,269] Trial 1 finished with value: 31.506664276123047 and parameters: {'n_estimators': 1519, 'learning_rate': 0.01259762168851857, 'max_depth': 4, 'min_child_weight': 0.40492333599412705, 'subsample': 0.7866397346298584, 'colsample_bytree': 0.9427179039843608, 'gamma': 3.5700272128073367, 'reg_alpha': 0.0004994658493833769, 'reg_lambda': 6.484485569090389}. Best is trial 1 with value: 31.506664276123047.\n",
      "[I 2025-10-25 10:28:13,328] Trial 2 finished with value: 9.877130508422852 and parameters: {'n_estimators': 1060, 'learning_rate': 0.29607190853592086, 'max_depth': 10, 'min_child_weight': 0.01792354591910649, 'subsample': 0.9561148194358157, 'colsample_bytree': 0.9780163498856038, 'gamma': 0.6241245503981668, 'reg_alpha': 0.0011897467722946976, 'reg_lambda': 0.008699038829623282}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:28:15,321] Trial 3 finished with value: 58.07373046875 and parameters: {'n_estimators': 594, 'learning_rate': 0.10299437561420995, 'max_depth': 3, 'min_child_weight': 7.0525388415586105, 'subsample': 0.8527459470104637, 'colsample_bytree': 0.883947688745643, 'gamma': 3.195615461247959, 'reg_alpha': 6.275320304356607e-07, 'reg_lambda': 9.19639107193077e-06}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:28:23,322] Trial 4 finished with value: 21.837398529052734 and parameters: {'n_estimators': 984, 'learning_rate': 0.09765582154370854, 'max_depth': 10, 'min_child_weight': 0.14920178717683988, 'subsample': 0.5560626621798912, 'colsample_bytree': 0.6903264734613312, 'gamma': 2.362880449669589, 'reg_alpha': 0.0037658624333318917, 'reg_lambda': 1.6294951711778457e-07}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:28:25,006] Trial 5 pruned. \n",
      "[I 2025-10-25 10:28:33,518] Trial 6 finished with value: 32.81903839111328 and parameters: {'n_estimators': 1076, 'learning_rate': 0.10622535286370519, 'max_depth': 7, 'min_child_weight': 0.058440906784864737, 'subsample': 0.5309917093903922, 'colsample_bytree': 0.7185715634441722, 'gamma': 0.3187524136935854, 'reg_alpha': 0.00018648309707656902, 'reg_lambda': 3.115530525742227e-05}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:28:43,316] Trial 7 finished with value: 12.432741165161133 and parameters: {'n_estimators': 1436, 'learning_rate': 0.05552781548503332, 'max_depth': 9, 'min_child_weight': 0.16487726450861667, 'subsample': 0.7356233494475155, 'colsample_bytree': 0.7563261254578522, 'gamma': 2.7246242711005935, 'reg_alpha': 0.0001830967156272408, 'reg_lambda': 2.1401240958827065e-06}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:28:52,823] Trial 8 finished with value: 15.920328140258789 and parameters: {'n_estimators': 1265, 'learning_rate': 0.035039563932658065, 'max_depth': 12, 'min_child_weight': 1.24339666719331, 'subsample': 0.9964555983850625, 'colsample_bytree': 0.8760452228980464, 'gamma': 1.931890374347628, 'reg_alpha': 0.000511810234480864, 'reg_lambda': 0.0009499775115414042}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:29:03,154] Trial 9 finished with value: 15.518594741821289 and parameters: {'n_estimators': 1080, 'learning_rate': 0.03183595837724514, 'max_depth': 8, 'min_child_weight': 0.4684291988899484, 'subsample': 0.7544726984103773, 'colsample_bytree': 0.7979238948746541, 'gamma': 4.897967683472871, 'reg_alpha': 1.9389246110700064e-08, 'reg_lambda': 0.025866506003074233}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:29:04,336] Trial 10 pruned. \n",
      "[I 2025-10-25 10:29:07,308] Trial 11 finished with value: 34.518272399902344 and parameters: {'n_estimators': 517, 'learning_rate': 0.26043381852842123, 'max_depth': 9, 'min_child_weight': 0.010458523408369082, 'subsample': 0.6763069571579593, 'colsample_bytree': 0.9808370390842784, 'gamma': 1.1705524473225997, 'reg_alpha': 0.06887222137695988, 'reg_lambda': 3.159498461184103e-08}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:29:07,836] Trial 12 pruned. \n",
      "[I 2025-10-25 10:29:09,880] Trial 13 pruned. \n",
      "[I 2025-10-25 10:29:11,180] Trial 14 pruned. \n",
      "[I 2025-10-25 10:29:15,092] Trial 15 finished with value: 13.558703422546387 and parameters: {'n_estimators': 1474, 'learning_rate': 0.16816262900159173, 'max_depth': 8, 'min_child_weight': 0.02791909228566514, 'subsample': 0.9049391981733914, 'colsample_bytree': 0.8782344143727916, 'gamma': 4.46464208804151, 'reg_alpha': 0.027127324839060545, 'reg_lambda': 7.516925990090106e-07}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:29:21,240] Trial 16 finished with value: 15.936349868774414 and parameters: {'n_estimators': 352, 'learning_rate': 0.06398014708987072, 'max_depth': 11, 'min_child_weight': 0.15117855309225398, 'subsample': 0.6213341433622097, 'colsample_bytree': 0.9985114261146557, 'gamma': 1.7635970686436906, 'reg_alpha': 1.8822113505375313, 'reg_lambda': 0.0017558065417107964}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:29:24,023] Trial 17 pruned. \n",
      "[I 2025-10-25 10:29:24,572] Trial 18 pruned. \n",
      "[I 2025-10-25 10:29:33,213] Trial 19 finished with value: 11.746014595031738 and parameters: {'n_estimators': 860, 'learning_rate': 0.049041381175633704, 'max_depth': 11, 'min_child_weight': 0.23645175351824219, 'subsample': 0.7287934828628568, 'colsample_bytree': 0.9371430841101701, 'gamma': 4.100125619606595, 'reg_alpha': 0.005158985831745363, 'reg_lambda': 7.413359362729899e-05}. Best is trial 2 with value: 9.877130508422852.\n",
      "[I 2025-10-25 10:29:46,613] Trial 20 finished with value: 9.143274307250977 and parameters: {'n_estimators': 885, 'learning_rate': 0.0250851744086728, 'max_depth': 11, 'min_child_weight': 0.6777296106765679, 'subsample': 0.8150260837654923, 'colsample_bytree': 0.9317414513337307, 'gamma': 4.06498495139297, 'reg_alpha': 0.5553648398493886, 'reg_lambda': 7.834086999950638e-05}. Best is trial 20 with value: 9.143274307250977.\n",
      "[I 2025-10-25 10:29:58,718] Trial 21 finished with value: 9.16554069519043 and parameters: {'n_estimators': 877, 'learning_rate': 0.02479811252641306, 'max_depth': 11, 'min_child_weight': 0.7598002972173398, 'subsample': 0.8333811389694522, 'colsample_bytree': 0.9319066733968812, 'gamma': 4.213398269965196, 'reg_alpha': 0.40685874734239597, 'reg_lambda': 0.00012420538322285518}. Best is trial 20 with value: 9.143274307250977.\n",
      "[I 2025-10-25 10:30:10,873] Trial 22 finished with value: 9.127603530883789 and parameters: {'n_estimators': 927, 'learning_rate': 0.025157666510317533, 'max_depth': 11, 'min_child_weight': 0.6821613222519798, 'subsample': 0.8138728528184519, 'colsample_bytree': 0.9370854313834021, 'gamma': 4.870399879593198, 'reg_alpha': 0.5498168840443574, 'reg_lambda': 0.006236769380687012}. Best is trial 22 with value: 9.127603530883789.\n",
      "[I 2025-10-25 10:30:19,718] Trial 23 finished with value: 32.71685791015625 and parameters: {'n_estimators': 672, 'learning_rate': 0.021913035087664276, 'max_depth': 11, 'min_child_weight': 2.6723340293120366, 'subsample': 0.805986757014836, 'colsample_bytree': 0.9114153152217197, 'gamma': 4.939100115564484, 'reg_alpha': 0.4613512336165242, 'reg_lambda': 0.0001225090259634446}. Best is trial 22 with value: 9.127603530883789.\n",
      "[I 2025-10-25 10:30:30,044] Trial 24 finished with value: 9.2378511428833 and parameters: {'n_estimators': 940, 'learning_rate': 0.03068546284569328, 'max_depth': 12, 'min_child_weight': 0.6512591426777704, 'subsample': 0.8411988569528702, 'colsample_bytree': 0.8412792518175287, 'gamma': 4.335060977278525, 'reg_alpha': 0.32908708990404145, 'reg_lambda': 0.0021342970234469374}. Best is trial 22 with value: 9.127603530883789.\n",
      "[I 2025-10-25 10:30:31,508] Trial 25 pruned. \n",
      "[I 2025-10-25 10:30:32,636] Trial 26 pruned. \n",
      "[I 2025-10-25 10:30:35,020] Trial 27 pruned. \n",
      "[I 2025-10-25 10:30:45,934] Trial 28 finished with value: 9.200200080871582 and parameters: {'n_estimators': 908, 'learning_rate': 0.025245860321862112, 'max_depth': 9, 'min_child_weight': 0.2735061337384368, 'subsample': 0.9231687695457808, 'colsample_bytree': 0.9557695289384444, 'gamma': 3.937673889697215, 'reg_alpha': 0.136174611503209, 'reg_lambda': 0.0050127250442955785}. Best is trial 22 with value: 9.127603530883789.\n",
      "[I 2025-10-25 10:30:55,669] Trial 29 finished with value: 28.667312622070312 and parameters: {'n_estimators': 622, 'learning_rate': 0.016231211565750737, 'max_depth': 10, 'min_child_weight': 0.7701369370764078, 'subsample': 0.8673722887693969, 'colsample_bytree': 0.7868285170444248, 'gamma': 4.499781539443633, 'reg_alpha': 0.02161434050533972, 'reg_lambda': 0.00025445518440067746}. Best is trial 22 with value: 9.127603530883789.\n",
      "[I 2025-10-25 10:30:57,042] Trial 30 pruned. \n",
      "[I 2025-10-25 10:31:07,087] Trial 31 finished with value: 9.277570724487305 and parameters: {'n_estimators': 903, 'learning_rate': 0.02550080479089341, 'max_depth': 9, 'min_child_weight': 0.4357944826431869, 'subsample': 0.9231479161268172, 'colsample_bytree': 0.9561487611644394, 'gamma': 3.947789086871157, 'reg_alpha': 0.11191871119911918, 'reg_lambda': 0.004703074298812875}. Best is trial 22 with value: 9.127603530883789.\n",
      "[I 2025-10-25 10:31:16,877] Trial 32 finished with value: 9.287137985229492 and parameters: {'n_estimators': 770, 'learning_rate': 0.03795037034788567, 'max_depth': 11, 'min_child_weight': 0.28247022582172565, 'subsample': 0.7833877041941346, 'colsample_bytree': 0.9017968314881445, 'gamma': 3.8517725660172317, 'reg_alpha': 0.10735261659034, 'reg_lambda': 0.000351093895184865}. Best is trial 22 with value: 9.127603530883789.\n",
      "[I 2025-10-25 10:31:31,047] Trial 33 finished with value: 9.049838066101074 and parameters: {'n_estimators': 887, 'learning_rate': 0.018412851101549315, 'max_depth': 12, 'min_child_weight': 0.08942689162124745, 'subsample': 0.9337864612801572, 'colsample_bytree': 0.9586223501497257, 'gamma': 4.3022192540721305, 'reg_alpha': 2.4147068297230514, 'reg_lambda': 0.027011720735492007}. Best is trial 33 with value: 9.049838066101074.\n",
      "[I 2025-10-25 10:31:32,760] Trial 34 pruned. \n",
      "[I 2025-10-25 10:31:34,016] Trial 35 pruned. \n",
      "[I 2025-10-25 10:31:35,456] Trial 36 pruned. \n",
      "[I 2025-10-25 10:31:37,909] Trial 37 pruned. \n",
      "[I 2025-10-25 10:31:52,703] Trial 38 finished with value: 9.140586853027344 and parameters: {'n_estimators': 1145, 'learning_rate': 0.019394411362744154, 'max_depth': 10, 'min_child_weight': 0.9759677029229668, 'subsample': 0.8320480750032075, 'colsample_bytree': 0.9053317151361084, 'gamma': 3.701569023237715, 'reg_alpha': 0.4715766487410203, 'reg_lambda': 0.0011056258596070655}. Best is trial 33 with value: 9.049838066101074.\n",
      "[I 2025-10-25 10:31:54,531] Trial 39 pruned. \n",
      "[I 2025-10-25 10:31:55,393] Trial 40 pruned. \n",
      "[I 2025-10-25 10:32:07,827] Trial 41 finished with value: 9.800971984863281 and parameters: {'n_estimators': 828, 'learning_rate': 0.028241885727499964, 'max_depth': 11, 'min_child_weight': 0.9708009371193258, 'subsample': 0.7702139393461743, 'colsample_bytree': 0.9746722258103975, 'gamma': 3.7382815264755713, 'reg_alpha': 0.25834881347446254, 'reg_lambda': 0.00012142946999629274}. Best is trial 33 with value: 9.049838066101074.\n",
      "[I 2025-10-25 10:32:10,441] Trial 42 pruned. \n",
      "[I 2025-10-25 10:32:11,545] Trial 43 pruned. \n",
      "[I 2025-10-25 10:32:25,185] Trial 44 finished with value: 9.342741012573242 and parameters: {'n_estimators': 934, 'learning_rate': 0.023909320187929708, 'max_depth': 12, 'min_child_weight': 0.5732768569600083, 'subsample': 0.8464821593249662, 'colsample_bytree': 0.862013298172157, 'gamma': 4.662597859812471, 'reg_alpha': 0.2101537723503851, 'reg_lambda': 0.149581297290301}. Best is trial 33 with value: 9.049838066101074.\n",
      "[I 2025-10-25 10:32:26,733] Trial 45 pruned. \n",
      "[I 2025-10-25 10:32:27,188] Trial 46 pruned. \n",
      "[I 2025-10-25 10:32:49,855] Trial 47 finished with value: 9.015694618225098 and parameters: {'n_estimators': 1635, 'learning_rate': 0.011357830666199348, 'max_depth': 10, 'min_child_weight': 0.953558564308704, 'subsample': 0.9128347562586963, 'colsample_bytree': 0.8953157826629276, 'gamma': 3.499555969891328, 'reg_alpha': 0.012521736818631172, 'reg_lambda': 0.003049668143793192}. Best is trial 47 with value: 9.015694618225098.\n",
      "[I 2025-10-25 10:32:52,107] Trial 48 pruned. \n",
      "[I 2025-10-25 10:32:54,502] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sMAPE: 9.015694618225098\n",
      "Best params: {'n_estimators': 1635, 'learning_rate': 0.011357830666199348, 'max_depth': 10, 'min_child_weight': 0.953558564308704, 'subsample': 0.9128347562586963, 'colsample_bytree': 0.8953157826629276, 'gamma': 3.499555969891328, 'reg_alpha': 0.012521736818631172, 'reg_lambda': 0.003049668143793192}\n"
     ]
    }
   ],
   "source": [
    "# ========= Optuna objective with 5-fold TimeSeriesSplit =========\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 2000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": 0,\n",
    "        # safe to include; ignored by the fallback path if needed\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    smapes = []\n",
    "\n",
    "    for tr_idx, va_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "        y_tr, y_val = y_train[tr_idx],    y_train[va_idx]\n",
    "\n",
    "        model = fit_xgb_compat(params, X_tr, y_tr, X_val, y_val, early_rounds=100)\n",
    "        yhat = model.predict(X_val)\n",
    "\n",
    "        # your sMAPE\n",
    "        denom = (np.abs(y_val) + np.abs(yhat) + 1e-9) / 2.0\n",
    "        s = np.mean(np.abs(yhat - y_val) / denom) * 100.0\n",
    "        smapes.append(s)\n",
    "\n",
    "        trial.report(float(np.mean(smapes)), step=len(smapes))\n",
    "        if trial.should_prune():\n",
    "            import optuna\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return float(np.mean(smapes))\n",
    "\n",
    "\n",
    "# ========= Run study =========\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"xgb-forecast-smape\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=False)  # adjust trials\n",
    "\n",
    "print(\"Best sMAPE:\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6d09b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | MAE: 1,710.771  RMSE: 7,300.389  sMAPE: 11.66%\n"
     ]
    }
   ],
   "source": [
    "# Fit with the compat wrapper from earlier\n",
    "final_model = fit_xgb_compat(best_params, X_train, y_train, X_val=X_test, y_val=y_test, early_rounds=200)\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "mae  = mean_absolute_error(y_test, y_pred_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred_test)\n",
    "smape_val = np.mean(np.abs(y_pred_test - y_test) / ((np.abs(y_test)+np.abs(y_pred_test)+1e-9)/2)) * 100\n",
    "print(f\"TEST | MAE: {mae:,.3f}  RMSE: {rmse:,.3f}  sMAPE: {smape_val:,.2f}%\")\n",
    "\n",
    "# Attach predictions back to rows\n",
    "test_out = test_df.copy()\n",
    "test_out[\"y_pred\"] = y_pred_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c2286",
   "metadata": {},
   "source": [
    "## Optimized – Modelling per product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:96: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  pd.api.types.is_categorical_dtype(out[c]) or\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:92: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(out[c]):\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:92: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(out[c]):\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:96: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  pd.api.types.is_categorical_dtype(out[c]) or\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:239: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(panel[c]) and c not in KEY_CATS]\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:96: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  pd.api.types.is_categorical_dtype(out[c]) or\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:92: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(out[c]):\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:125: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[c] = (df.groupby(\"admin_1\", group_keys=False)[c]\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:155: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby([\"admin_1\", \"product\"], group_keys=False)[\"y\"]\n",
      "[I 2025-10-25 12:29:35,014] A new study created in memory with name: xgb-staples-smape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning staples model with 40 trials…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 12:29:37,166] Trial 0 finished with value: 1.883623362151949 and parameters: {'n_estimators': 1162, 'learning_rate': 0.015683503930815343, 'max_depth': 5, 'min_child_weight': 6.771436939990866, 'subsample': 0.8017806293002239, 'colsample_bytree': 0.8738505559082057, 'gamma': 1.2624277917206517, 'reg_alpha': 0.0001254085050201393, 'reg_lambda': 0.0011405480670603888}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:38,103] Trial 1 finished with value: 2.4424633156803943 and parameters: {'n_estimators': 670, 'learning_rate': 0.21083476708743015, 'max_depth': 5, 'min_child_weight': 0.17063720795201975, 'subsample': 0.7960110908234259, 'colsample_bytree': 0.5672388230881045, 'gamma': 2.341630767169926, 'reg_alpha': 0.03434704104250717, 'reg_lambda': 2.0065401868252615e-08}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:39,850] Trial 2 finished with value: 2.6928124047718223 and parameters: {'n_estimators': 1062, 'learning_rate': 0.17461596394004497, 'max_depth': 8, 'min_child_weight': 0.792209693117398, 'subsample': 0.7583828786209108, 'colsample_bytree': 0.514050682103849, 'gamma': 4.0658206342302785, 'reg_alpha': 1.8598425904042762e-06, 'reg_lambda': 3.167833618317898e-08}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:42,640] Trial 3 finished with value: 2.5623113138319624 and parameters: {'n_estimators': 1511, 'learning_rate': 0.10972145075151653, 'max_depth': 8, 'min_child_weight': 0.0957979620335752, 'subsample': 0.6065931690608648, 'colsample_bytree': 0.5167172601141528, 'gamma': 4.966235301367167, 'reg_alpha': 2.3287054558181722e-05, 'reg_lambda': 0.006001686114373946}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:45,246] Trial 4 finished with value: 2.0483405720612056 and parameters: {'n_estimators': 1903, 'learning_rate': 0.09149943893724838, 'max_depth': 3, 'min_child_weight': 2.5377457211557077, 'subsample': 0.9628497137992216, 'colsample_bytree': 0.6097196329664853, 'gamma': 1.038413224166213, 'reg_alpha': 4.678094636012927e-05, 'reg_lambda': 9.035739669524134e-06}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:46,040] Trial 5 pruned. \n",
      "[I 2025-10-25 12:29:48,426] Trial 6 finished with value: 2.045904285030571 and parameters: {'n_estimators': 1430, 'learning_rate': 0.015838544136757508, 'max_depth': 7, 'min_child_weight': 6.358569333177176, 'subsample': 0.7983397717085782, 'colsample_bytree': 0.7736083893248795, 'gamma': 3.3948750454621073, 'reg_alpha': 2.908440578685106e-05, 'reg_lambda': 1.1548003922310149e-07}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:51,610] Trial 7 finished with value: 2.152721665311726 and parameters: {'n_estimators': 1438, 'learning_rate': 0.02127267218714779, 'max_depth': 11, 'min_child_weight': 0.022596068192070013, 'subsample': 0.7272730351857866, 'colsample_bytree': 0.5125144217716124, 'gamma': 2.2871865959709, 'reg_alpha': 0.07706467499003977, 'reg_lambda': 7.741267229327297e-05}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:51,836] Trial 8 pruned. \n",
      "[I 2025-10-25 12:29:53,212] Trial 9 finished with value: 2.0334378049251125 and parameters: {'n_estimators': 913, 'learning_rate': 0.06446232977496562, 'max_depth': 5, 'min_child_weight': 0.03159094500360968, 'subsample': 0.7023914833193527, 'colsample_bytree': 0.8201392529331433, 'gamma': 2.6831409307648952, 'reg_alpha': 2.1744650564123865e-07, 'reg_lambda': 1.8931917161467518e-06}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:54,279] Trial 10 finished with value: 1.8986723702919792 and parameters: {'n_estimators': 328, 'learning_rate': 0.02595561697776664, 'max_depth': 3, 'min_child_weight': 0.8463909122442486, 'subsample': 0.9228022868912353, 'colsample_bytree': 0.9981475723109499, 'gamma': 0.01628494719522311, 'reg_alpha': 6.5929006802485555, 'reg_lambda': 6.228474769520813}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:55,174] Trial 11 finished with value: 1.9206469125206638 and parameters: {'n_estimators': 309, 'learning_rate': 0.028607597975790672, 'max_depth': 3, 'min_child_weight': 0.7186621335408947, 'subsample': 0.9486706188850782, 'colsample_bytree': 0.9997497614327149, 'gamma': 0.046738153634140245, 'reg_alpha': 9.537786149839567, 'reg_lambda': 8.449456713596867}. Best is trial 0 with value: 1.883623362151949.\n",
      "[I 2025-10-25 12:29:57,750] Trial 12 finished with value: 1.8668749719532642 and parameters: {'n_estimators': 401, 'learning_rate': 0.012022274246543601, 'max_depth': 4, 'min_child_weight': 3.7871664604098796, 'subsample': 0.8656010914331116, 'colsample_bytree': 0.9751749153214203, 'gamma': 0.24590641127390273, 'reg_alpha': 0.003104448572148554, 'reg_lambda': 0.15016067767643637}. Best is trial 12 with value: 1.8668749719532642.\n",
      "[I 2025-10-25 12:29:59,264] Trial 13 finished with value: 1.9062593843637796 and parameters: {'n_estimators': 549, 'learning_rate': 0.010522441193619432, 'max_depth': 5, 'min_child_weight': 8.52833508153686, 'subsample': 0.5103158793683393, 'colsample_bytree': 0.9044047976420018, 'gamma': 1.1227708601146633, 'reg_alpha': 0.0016530991457870984, 'reg_lambda': 0.02727285572329213}. Best is trial 12 with value: 1.8668749719532642.\n",
      "[I 2025-10-25 12:30:01,733] Trial 14 finished with value: 1.8546787860921907 and parameters: {'n_estimators': 1160, 'learning_rate': 0.010147105855020292, 'max_depth': 11, 'min_child_weight': 2.721548647419669, 'subsample': 0.8624592093069995, 'colsample_bytree': 0.9140573163613324, 'gamma': 0.9981424052264198, 'reg_alpha': 0.00218904753164398, 'reg_lambda': 0.07224440197322216}. Best is trial 14 with value: 1.8546787860921907.\n",
      "[I 2025-10-25 12:30:04,391] Trial 15 finished with value: 1.816558140938205 and parameters: {'n_estimators': 1180, 'learning_rate': 0.010157144474044902, 'max_depth': 12, 'min_child_weight': 2.209080061372453, 'subsample': 0.8743793224002957, 'colsample_bytree': 0.9263701117980992, 'gamma': 0.6651475510044987, 'reg_alpha': 0.002708058293310349, 'reg_lambda': 0.3532944710094998}. Best is trial 15 with value: 1.816558140938205.\n",
      "[I 2025-10-25 12:30:05,086] Trial 16 pruned. \n",
      "[I 2025-10-25 12:30:07,763] Trial 17 finished with value: 1.8142061820901094 and parameters: {'n_estimators': 1752, 'learning_rate': 0.03915791565218528, 'max_depth': 10, 'min_child_weight': 1.3210488741047735, 'subsample': 0.8667504772440406, 'colsample_bytree': 0.9150204106130793, 'gamma': 0.6790437660983134, 'reg_alpha': 2.0739086138890903e-08, 'reg_lambda': 0.3632129430339423}. Best is trial 17 with value: 1.8142061820901094.\n",
      "[I 2025-10-25 12:30:10,726] Trial 18 finished with value: 1.9108582381061994 and parameters: {'n_estimators': 1995, 'learning_rate': 0.04010813506691136, 'max_depth': 10, 'min_child_weight': 0.3618378196169285, 'subsample': 0.9913471577955617, 'colsample_bytree': 0.6957011143841337, 'gamma': 0.6088634879059094, 'reg_alpha': 4.414700938687689e-08, 'reg_lambda': 0.7279153362873783}. Best is trial 17 with value: 1.8142061820901094.\n",
      "[I 2025-10-25 12:30:12,199] Trial 19 pruned. \n",
      "[I 2025-10-25 12:30:14,774] Trial 20 finished with value: 1.8136489407741485 and parameters: {'n_estimators': 1711, 'learning_rate': 0.043548854114994345, 'max_depth': 12, 'min_child_weight': 0.18984081099452357, 'subsample': 0.8512418019332648, 'colsample_bytree': 0.7800810984267526, 'gamma': 0.6459778129703139, 'reg_alpha': 3.865525022610655e-06, 'reg_lambda': 0.7975735353181777}. Best is trial 20 with value: 1.8136489407741485.\n",
      "[I 2025-10-25 12:30:17,949] Trial 21 finished with value: 1.8320167133618184 and parameters: {'n_estimators': 1693, 'learning_rate': 0.042882025167913444, 'max_depth': 12, 'min_child_weight': 0.3493810425441425, 'subsample': 0.828977560958105, 'colsample_bytree': 0.7841600184154648, 'gamma': 0.6298055292018881, 'reg_alpha': 3.984039038718659e-06, 'reg_lambda': 1.1573182151366364}. Best is trial 20 with value: 1.8136489407741485.\n",
      "[I 2025-10-25 12:30:20,773] Trial 22 finished with value: 1.8647880031767614 and parameters: {'n_estimators': 1711, 'learning_rate': 0.020506279666162384, 'max_depth': 10, 'min_child_weight': 0.07444288829999933, 'subsample': 0.8347175627633641, 'colsample_bytree': 0.7011945269695334, 'gamma': 0.6291326523225986, 'reg_alpha': 1.7763452199499165e-08, 'reg_lambda': 0.01648106687082992}. Best is trial 20 with value: 1.8136489407741485.\n",
      "[I 2025-10-25 12:30:21,262] Trial 23 pruned. \n",
      "[I 2025-10-25 12:30:21,613] Trial 24 pruned. \n",
      "[I 2025-10-25 12:30:26,649] Trial 25 finished with value: 1.852198807189015 and parameters: {'n_estimators': 1624, 'learning_rate': 0.030578486617981456, 'max_depth': 9, 'min_child_weight': 0.23023240578627618, 'subsample': 0.8353267152416218, 'colsample_bytree': 0.7368699282654293, 'gamma': 0.5360793531301521, 'reg_alpha': 5.631521928488705e-06, 'reg_lambda': 0.0017645401902976197}. Best is trial 20 with value: 1.8136489407741485.\n",
      "[I 2025-10-25 12:30:28,175] Trial 26 finished with value: 1.8905082932040376 and parameters: {'n_estimators': 964, 'learning_rate': 0.06949990345673619, 'max_depth': 11, 'min_child_weight': 0.46995405531066425, 'subsample': 0.931856410937326, 'colsample_bytree': 0.8065745756140803, 'gamma': 0.8163265170067744, 'reg_alpha': 1.837580544890647e-07, 'reg_lambda': 2.768523317227049}. Best is trial 20 with value: 1.8136489407741485.\n",
      "[I 2025-10-25 12:30:28,641] Trial 27 pruned. \n",
      "[I 2025-10-25 12:30:30,660] Trial 28 pruned. \n",
      "[I 2025-10-25 12:30:31,578] Trial 29 pruned. \n",
      "[I 2025-10-25 12:30:31,931] Trial 30 pruned. \n",
      "[I 2025-10-25 12:30:34,488] Trial 31 finished with value: 1.8693898294566131 and parameters: {'n_estimators': 1694, 'learning_rate': 0.03976641751191788, 'max_depth': 12, 'min_child_weight': 0.3384879454693276, 'subsample': 0.8223102888394812, 'colsample_bytree': 0.7808642150739633, 'gamma': 0.9078439650827941, 'reg_alpha': 3.960039894908097e-06, 'reg_lambda': 1.2377612864691678}. Best is trial 20 with value: 1.8136489407741485.\n",
      "[I 2025-10-25 12:30:37,508] Trial 32 finished with value: 1.81234887223115 and parameters: {'n_estimators': 1985, 'learning_rate': 0.044651435838751405, 'max_depth': 12, 'min_child_weight': 1.100802788616365, 'subsample': 0.7834092235268509, 'colsample_bytree': 0.7402039544143411, 'gamma': 0.3075592909068132, 'reg_alpha': 0.0001003344709333014, 'reg_lambda': 0.12276444778368853}. Best is trial 32 with value: 1.81234887223115.\n",
      "[I 2025-10-25 12:30:41,695] Trial 33 finished with value: 1.7865611823371488 and parameters: {'n_estimators': 1999, 'learning_rate': 0.03445348757177523, 'max_depth': 11, 'min_child_weight': 1.0705260182528338, 'subsample': 0.7926410897298273, 'colsample_bytree': 0.7273866944456198, 'gamma': 0.15093154465589476, 'reg_alpha': 0.0001681070059026855, 'reg_lambda': 0.11063442723654303}. Best is trial 33 with value: 1.7865611823371488.\n",
      "[I 2025-10-25 12:30:45,662] Trial 34 finished with value: 1.8391291487128192 and parameters: {'n_estimators': 1980, 'learning_rate': 0.033785272110306346, 'max_depth': 11, 'min_child_weight': 1.2429075455114167, 'subsample': 0.7669214075583074, 'colsample_bytree': 0.727533016698624, 'gamma': 0.2628594570708877, 'reg_alpha': 9.563791584924611e-05, 'reg_lambda': 0.00918416119818527}. Best is trial 33 with value: 1.7865611823371488.\n",
      "[I 2025-10-25 12:30:50,106] Trial 35 finished with value: 1.916706182485895 and parameters: {'n_estimators': 1799, 'learning_rate': 0.04715407524111956, 'max_depth': 11, 'min_child_weight': 0.9659387518939027, 'subsample': 0.7213450578553118, 'colsample_bytree': 0.6518298924237744, 'gamma': 0.014678506624425713, 'reg_alpha': 0.0004360009890420413, 'reg_lambda': 0.0034564297178868823}. Best is trial 33 with value: 1.7865611823371488.\n",
      "[I 2025-10-25 12:30:50,665] Trial 36 pruned. \n",
      "[I 2025-10-25 12:30:51,434] Trial 37 pruned. \n",
      "[I 2025-10-25 12:30:51,901] Trial 38 pruned. \n",
      "[I 2025-10-25 12:30:52,470] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sMAPE: 1.7865611823371488\n",
      "Best params: {'n_estimators': 1999, 'learning_rate': 0.03445348757177523, 'max_depth': 11, 'min_child_weight': 1.0705260182528338, 'subsample': 0.7926410897298273, 'colsample_bytree': 0.7273866944456198, 'gamma': 0.15093154465589476, 'reg_alpha': 0.0001681070059026855, 'reg_lambda': 0.11063442723654303}\n",
      "\n",
      "=== STAPLES TEST METRICS ===\n",
      "MAE : 12.274\n",
      "RMSE: 29.607\n",
      "sMAPE: 7.57%\n",
      "\n",
      "Worst staples by sMAPE:\n",
      "               product     sMAPE       MAE      RMSE    N\n",
      " Refined Vegetable Oil 28.391415 79.454699 87.775995 35.0\n",
      "         Sorghum (Red)  7.548771  4.350891  6.525895 32.0\n",
      "       Sorghum (White)  6.597296  4.340350  7.546356 26.0\n",
      "   Maize Grain (White)  5.499172  2.690474  4.021647 56.0\n",
      "            Mixed Teff  5.351611  6.647818  7.193303 24.0\n",
      "         Rice (Milled)  5.057807  6.475950 10.780830 35.0\n",
      "           Wheat Flour  4.356458  4.394201  7.740979 35.0\n",
      "           Wheat Grain  3.599267  2.599030  3.600205 56.0\n",
      "         Refined sugar  2.712448  3.609062  5.577724 23.0\n",
      "Camels (Local Quality)       NaN       NaN       NaN  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/2457992708.py:314: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(metrics_for_group)\n"
     ]
    }
   ],
   "source": [
    "# ===================== STAPLES-ONLY PRICE FORECAST PIPELINE =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---- config ----\n",
    "PARQUET_PATH = \"/Users/nataschajademinnitt/Documents/5_data/food_security/etl/ethiopia_foodprices_model_panel.parquet\"\n",
    "TEST_HORIZON_MONTHS = 6      # last H calendar months = test\n",
    "N_TRIALS = 40                # Optuna trials (adjust up for more thorough search)\n",
    "\n",
    "# ---- helpers ----\n",
    "def month_start(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-9) -> float:\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return float(np.mean(np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols: list[str], dtype=\"float64\") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            out[c] = np.nan\n",
    "        if dtype is not None:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                try:\n",
    "                    out[c] = out[c].astype(dtype)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return out\n",
    "\n",
    "def fit_xgb_compat(params, X_tr, y_tr, X_val=None, y_val=None, early_rounds=100):\n",
    "    \"\"\"Try XGBoost >=2 callbacks; fall back to <2.0 API; last resort w/o early stopping.\"\"\"\n",
    "    # Try newer API\n",
    "    try:\n",
    "        from xgboost.callback import EarlyStopping\n",
    "        mdl = XGBRegressor(**params)\n",
    "        if X_val is not None:\n",
    "            mdl.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[EarlyStopping(rounds=early_rounds, save_best=True)])\n",
    "        else:\n",
    "            mdl.fit(X_tr, y_tr)\n",
    "        return mdl\n",
    "    except (TypeError, ImportError):\n",
    "        pass\n",
    "    # Fallback: older API\n",
    "    try:\n",
    "        mdl = XGBRegressor(**{k: v for k, v in params.items() if k != \"eval_metric\"})\n",
    "        if X_val is not None:\n",
    "            mdl.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                    eval_metric=params.get(\"eval_metric\", \"rmse\"),\n",
    "                    early_stopping_rounds=early_rounds)\n",
    "        else:\n",
    "            mdl.fit(X_tr, y_tr)\n",
    "        return mdl\n",
    "    except TypeError:\n",
    "        pass\n",
    "    # Last resort\n",
    "    mdl = XGBRegressor(**{k: v for k, v in params.items() if k != \"eval_metric\"})\n",
    "    mdl.fit(X_tr, y_tr)\n",
    "    return mdl\n",
    "\n",
    "# ---- de-categorize numerics (prevents 'new category' errors) ----\n",
    "KEY_CATS = {\"admin_1\", \"product\"}\n",
    "NUMERIC_LIKELY = {\n",
    "    \"value_imputed\",\"value_mean\",\"value_median\",\"value_orig\",\"n_obs\",\n",
    "    \"rfh_month\",\"rfh_avg_month\",\"rfq_month\",\"rain_anom_pct\",\n",
    "    \"fao_category_index\",\"fao_food_price_index\",\n",
    "    \"ptm_severity\",\"population_2023\",\n",
    "    \"y\",\"y_lag1\",\"y_lag3\",\"y_lag6\",\"y_lag12\",\n",
    "    \"roll_mean_3\",\"roll_std_3\",\"roll_mean_12\",\n",
    "    \"month_num\",\"mo_sin\",\"mo_cos\",\n",
    "}\n",
    "\n",
    "def decategorize_numerics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if c in KEY_CATS:\n",
    "            # keep IDs as category\n",
    "            if not pd.api.types.is_categorical_dtype(out[c]):\n",
    "                out[c] = out[c].astype(\"category\")\n",
    "            continue\n",
    "        if (c in NUMERIC_LIKELY or\n",
    "            pd.api.types.is_categorical_dtype(out[c]) or\n",
    "            pd.api.types.is_object_dtype(out[c])):\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "# ---- feature imputation for optional exogenous vars ----\n",
    "def impute_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = decategorize_numerics(df_in)\n",
    "    df[\"month\"] = month_start(df[\"month\"])\n",
    "\n",
    "    rain_cols = [\"rfh_month\", \"rfh_avg_month\", \"rfq_month\", \"rain_anom_pct\"]\n",
    "    fao_cols  = [\"fao_category_index\", \"fao_food_price_index\"]\n",
    "    df = ensure_cols(df, rain_cols + fao_cols)\n",
    "\n",
    "    # missingness flags first\n",
    "    for c in rain_cols:\n",
    "        df[f\"{c}_was_missing\"] = df[c].isna().astype(\"int8\")\n",
    "\n",
    "    # recompute anomaly where possible\n",
    "    can = df[\"rfh_month\"].notna() & df[\"rfh_avg_month\"].gt(0)\n",
    "    df.loc[can, \"rain_anom_pct\"] = 100.0 * (df.loc[can, \"rfh_month\"] / df.loc[can, \"rfh_avg_month\"] - 1.0)\n",
    "\n",
    "    # same-month median fill\n",
    "    for c in rain_cols + fao_cols:\n",
    "        df[c] = df[c].fillna(df.groupby(\"month\", observed=False)[c].transform(\"median\"))\n",
    "\n",
    "    # gentle past-only rolling fill by admin\n",
    "    df = df.sort_values([\"admin_1\", \"month\"])\n",
    "    for c in rain_cols:\n",
    "        df[c] = (df.groupby(\"admin_1\", group_keys=False)[c]\n",
    "                   .apply(lambda s: s.fillna(s.shift(1).rolling(3, min_periods=1).mean())))\n",
    "\n",
    "    # GMM & population (if present)\n",
    "    if \"ptm_severity\" in df.columns:\n",
    "        df[\"ptm_missing\"] = df[\"ptm_severity\"].isna().astype(\"int8\")\n",
    "        df[\"ptm_severity\"] = df[\"ptm_severity\"].fillna(0).astype(\"int8\")\n",
    "    else:\n",
    "        df[\"ptm_missing\"] = 1; df[\"ptm_severity\"] = 0\n",
    "\n",
    "    if \"population_2023\" in df.columns:\n",
    "        df[\"pop_missing\"] = df[\"population_2023\"].isna().astype(\"int8\")\n",
    "        adm_med = df.groupby(\"admin_1\", observed=False)[\"population_2023\"].transform(\"median\")\n",
    "        df[\"population_2023\"] = df[\"population_2023\"].fillna(adm_med)\n",
    "        df[\"population_2023\"] = df[\"population_2023\"].fillna(df[\"population_2023\"].median())\n",
    "    else:\n",
    "        df[\"pop_missing\"] = 1; df[\"population_2023\"] = np.nan\n",
    "\n",
    "    # FAO forward-fill by time\n",
    "    df = df.sort_values([\"month\"])\n",
    "    for c in fao_cols:\n",
    "        df[c] = df[c].ffill()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---- build lag/roll + seasonal features (per admin_1, product) ----\n",
    "def build_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.sort_values([\"admin_1\", \"product\", \"month\"]).copy()\n",
    "    df[\"y\"] = df[\"value_imputed\"].astype(\"float32\")\n",
    "\n",
    "    g = df.groupby([\"admin_1\", \"product\"], group_keys=False)[\"y\"]\n",
    "    df[\"y_lag1\"]  = g.shift(1)\n",
    "    df[\"y_lag3\"]  = g.shift(3)\n",
    "    df[\"y_lag6\"]  = g.shift(6)\n",
    "    df[\"y_lag12\"] = g.shift(12)\n",
    "\n",
    "    df[\"roll_mean_3\"]  = g.apply(lambda s: s.shift(1).rolling(3,  min_periods=1).mean())\n",
    "    df[\"roll_std_3\"]   = g.apply(lambda s: s.shift(1).rolling(3,  min_periods=2).std())\n",
    "    df[\"roll_mean_12\"] = g.apply(lambda s: s.shift(1).rolling(12, min_periods=3).mean())\n",
    "\n",
    "    df[\"month_num\"] = df[\"month\"].dt.month\n",
    "    df[\"mo_sin\"] = np.sin(2*np.pi*df[\"month_num\"]/12.0)\n",
    "    df[\"mo_cos\"] = np.cos(2*np.pi*df[\"month_num\"]/12.0)\n",
    "\n",
    "    # require minimal history\n",
    "    df = df[df[\"y_lag1\"].notna()].copy()\n",
    "    return df\n",
    "\n",
    "def encode_ids(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in [\"admin_1\", \"product\"]:\n",
    "        out[c] = out[c].astype(\"category\")\n",
    "        out[f\"{c}_code\"] = out[c].cat.codes.astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def pick_features(df: pd.DataFrame) -> list[str]:\n",
    "    drop = {\n",
    "        \"y\", \"month\", \"admin_1\", \"product\",\n",
    "        \"value_imputed\", \"value_median\", \"value_mean\", \"value_orig\",\n",
    "        \"n_obs\", \"impute_method\", \"sources\", \"period_date\"\n",
    "    }\n",
    "    return [c for c in df.columns if c not in drop]\n",
    "\n",
    "def time_split(df: pd.DataFrame, horizon=6):\n",
    "    df = df.sort_values([\"month\", \"admin_1\", \"product\"]).reset_index(drop=True)\n",
    "    last_m = df[\"month\"].max().to_period(\"M\").to_timestamp()\n",
    "    cut_m  = (last_m.to_period(\"M\") - horizon).to_timestamp()\n",
    "    return df[df[\"month\"] <= cut_m].copy(), df[df[\"month\"] > cut_m].copy()\n",
    "\n",
    "# ---- Optuna tuning (time-aware CV; assumes X_train already time-sorted) ----\n",
    "def tune_xgb(X_train, y_train, n_trials=40, seed=42):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 2000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "            \"tree_method\": \"hist\", \"random_state\": seed, \"n_jobs\": -1, \"eval_metric\": \"rmse\",\n",
    "        }\n",
    "        smapes = []\n",
    "        for tr_idx, va_idx in tscv.split(X_train):\n",
    "            X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "            y_tr, y_val = y_train[tr_idx],    y_train[va_idx]\n",
    "            mdl = fit_xgb_compat(params, X_tr, y_tr, X_val, y_val, early_rounds=100)\n",
    "            smapes.append(smape(y_val, mdl.predict(X_val)))\n",
    "            trial.report(float(np.mean(smapes)), step=len(smapes))\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        return float(np.mean(smapes))\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=\"xgb-staples-smape\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    print(\"Best sMAPE:\", study.best_value)\n",
    "    print(\"Best params:\", study.best_trial.params)\n",
    "    best = study.best_trial.params\n",
    "    best.update({\"tree_method\": \"hist\", \"random_state\": seed, \"n_jobs\": -1, \"eval_metric\": \"rmse\"})\n",
    "    return best\n",
    "\n",
    "# ============================== RUN STAPLES ONLY ===============================\n",
    "# 1) Load\n",
    "panel = pd.read_parquet(PARQUET_PATH)\n",
    "assert {\"admin_1\", \"product\", \"month\", \"value_imputed\"}.issubset(panel.columns)\n",
    "\n",
    "# De-categorize numerics once after load\n",
    "panel = decategorize_numerics(panel)\n",
    "# Force any stray non-key categoricals to numeric, just in case\n",
    "suspect_cats = [c for c in panel.columns\n",
    "                if pd.api.types.is_categorical_dtype(panel[c]) and c not in KEY_CATS]\n",
    "for c in suspect_cats:\n",
    "    panel[c] = pd.to_numeric(panel[c].astype(object), errors=\"coerce\")\n",
    "\n",
    "# 2) Drop livestock\n",
    "LIVESTOCK = {\n",
    "    \"Goats (Local Quality)\", \"Sheep (Local Quality)\",\n",
    "    \"Camels (Local Quality)\", \"Oxen (Local Quality)\"\n",
    "}\n",
    "staples = panel[~panel[\"product\"].isin(LIVESTOCK)].copy()\n",
    "\n",
    "# 3) Feature imputations & matrix\n",
    "staples = impute_features(staples)\n",
    "df_feat  = build_features(staples)\n",
    "df_feat  = encode_ids(df_feat)\n",
    "\n",
    "# 4) Split & features\n",
    "df_feat = df_feat.sort_values([\"month\", \"admin_1\", \"product\"]).reset_index(drop=True)\n",
    "feats = pick_features(df_feat)\n",
    "df_feat[feats] = df_feat[feats].apply(pd.to_numeric, errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "train_df, test_df = time_split(df_feat, horizon=TEST_HORIZON_MONTHS)\n",
    "X_train, X_test = train_df[feats], test_df[feats]\n",
    "\n",
    "# --- log target\n",
    "y_train_log = np.log1p(train_df[\"y\"].to_numpy())\n",
    "y_test_true = test_df[\"y\"].to_numpy()\n",
    "\n",
    "# 5) Tune on TRAIN\n",
    "print(f\"Tuning staples model with {N_TRIALS} trials…\")\n",
    "best_params = tune_xgb(X_train, y_train_log, n_trials=N_TRIALS, seed=42)\n",
    "\n",
    "# 6) Early stopping on tail of TRAIN (no leakage)\n",
    "last_train_m = train_df[\"month\"].max()\n",
    "val_cut = (last_train_m.to_period(\"M\") - 2).to_timestamp()  # last 3 months as ES\n",
    "val_mask = train_df[\"month\"] >= val_cut\n",
    "X_tr_es, y_tr_es = X_train[~val_mask], y_train_log[~val_mask]\n",
    "X_va_es, y_va_es = X_train[val_mask],  y_train_log[val_mask]\n",
    "if len(X_va_es) == 0 or len(X_tr_es) == 0:\n",
    "    X_tr_es, y_tr_es = X_train, y_train_log\n",
    "    X_va_es = y_va_es = None\n",
    "\n",
    "# 7) Fit & predict (invert log)\n",
    "final_model = fit_xgb_compat(best_params, X_tr_es, y_tr_es, X_va_es, y_va_es, early_rounds=200)\n",
    "y_pred_test = np.expm1(final_model.predict(X_test))\n",
    "\n",
    "# 8) Light, product-wise caps (optional but robust)\n",
    "lo = (train_df.groupby(\"product\", observed=False)[\"y\"].quantile(0.01))\n",
    "hi = (train_df.groupby(\"product\", observed=False)[\"y\"].quantile(0.99))\n",
    "m = test_df[\"product\"].map(lo).fillna(train_df[\"y\"].quantile(0.01)) * 0.5\n",
    "M = test_df[\"product\"].map(hi).fillna(train_df[\"y\"].quantile(0.99)) * 2.0\n",
    "y_pred_test = np.clip(y_pred_test, m.to_numpy(), M.to_numpy())\n",
    "\n",
    "# 9) Metrics & per-product breakdown\n",
    "print(\"\\n=== STAPLES TEST METRICS ===\")\n",
    "print(f\"MAE : {mean_absolute_error(y_test_true, y_pred_test):,.3f}\")\n",
    "print(f\"RMSE: {rmse(y_test_true, y_pred_test):,.3f}\")\n",
    "print(f\"sMAPE: {smape(y_test_true, y_pred_test):.2f}%\")\n",
    "\n",
    "# attach predictions\n",
    "out_df = test_df.copy()\n",
    "out_df[\"y_pred\"] = y_pred_test\n",
    "\n",
    "def metrics_for_group(g: pd.DataFrame) -> pd.Series:\n",
    "    gg = g.loc[g[\"y_pred\"].notna() & g[\"y\"].notna(), [\"y\", \"y_pred\"]]\n",
    "    n = int(len(gg))\n",
    "    if n == 0:\n",
    "        return pd.Series({\"sMAPE\": np.nan, \"MAE\": np.nan, \"RMSE\": np.nan, \"N\": 0})\n",
    "    mae  = float(np.abs(gg[\"y\"] - gg[\"y_pred\"]).mean())\n",
    "    rmse_v = float(np.sqrt(((gg[\"y\"] - gg[\"y_pred\"])**2).mean()))\n",
    "    denom = (np.abs(gg[\"y\"]) + np.abs(gg[\"y_pred\"]) + 1e-9) / 2.0\n",
    "    sm   = float((np.abs(gg[\"y_pred\"] - gg[\"y\"]) / denom).mean() * 100.0)\n",
    "    return pd.Series({\"sMAPE\": sm, \"MAE\": mae, \"RMSE\": rmse_v, \"N\": n})\n",
    "\n",
    "by_prod = (out_df.groupby(\"product\", observed=False)\n",
    "                 .apply(metrics_for_group)\n",
    "                 .reset_index()\n",
    "                 .sort_values(\"sMAPE\", ascending=False))\n",
    "\n",
    "print(\"\\nWorst staples by sMAPE:\")\n",
    "print(by_prod.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc99a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:59: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  pd.api.types.is_categorical_dtype(out[c]) or\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:55: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(out[c]):\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:55: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(out[c]):\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:59: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  pd.api.types.is_categorical_dtype(out[c]) or\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:324: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(panel[c]) and c not in KEY_CATS:\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:59: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  pd.api.types.is_categorical_dtype(out[c]) or\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:55: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(out[c]):\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:131: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[c] = (df.groupby(\"admin_1\", group_keys=False)[c]\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:160: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby([\"admin_1\",\"product\"], group_keys=False)[\"y\"]\n",
      "[I 2025-10-25 12:34:20,467] A new study created in memory with name: xgb-staples-smape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-25 12:34:20.465786] Tuning (40 trials)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 12:34:23,059] Trial 0 finished with value: 2.5909003967349946 and parameters: {'n_estimators': 1610, 'learning_rate': 0.276206748644435, 'max_depth': 7, 'min_child_weight': 0.7001440005267261, 'subsample': 0.7064562857829264, 'colsample_bytree': 0.6412959759197572, 'gamma': 2.9111580514058715, 'reg_alpha': 0.00019003726177874546, 'reg_lambda': 0.20112727332356684}. Best is trial 0 with value: 2.5909003967349946.\n",
      "[I 2025-10-25 12:34:27,866] Trial 1 finished with value: 2.144116314439753 and parameters: {'n_estimators': 1928, 'learning_rate': 0.01667350193709267, 'max_depth': 10, 'min_child_weight': 0.03767687709154869, 'subsample': 0.9751628442076601, 'colsample_bytree': 0.9258384879077287, 'gamma': 4.479894224603827, 'reg_alpha': 0.00018975561123092013, 'reg_lambda': 9.731188034963155e-05}. Best is trial 1 with value: 2.144116314439753.\n",
      "[I 2025-10-25 12:34:29,351] Trial 2 finished with value: 2.0730644744538704 and parameters: {'n_estimators': 921, 'learning_rate': 0.030004193493791537, 'max_depth': 3, 'min_child_weight': 0.01989940031343092, 'subsample': 0.8639114275877683, 'colsample_bytree': 0.565837692047734, 'gamma': 2.244561424940663, 'reg_alpha': 0.016232252387564185, 'reg_lambda': 5.915038925693058e-08}. Best is trial 2 with value: 2.0730644744538704.\n",
      "[I 2025-10-25 12:34:30,427] Trial 3 finished with value: 1.9794415155342833 and parameters: {'n_estimators': 571, 'learning_rate': 0.030797641908718563, 'max_depth': 12, 'min_child_weight': 0.08672586490542414, 'subsample': 0.7208211760101706, 'colsample_bytree': 0.9675677563543884, 'gamma': 2.0470647557965465, 'reg_alpha': 0.0001341430907250179, 'reg_lambda': 0.15027730474008988}. Best is trial 3 with value: 1.9794415155342833.\n",
      "[I 2025-10-25 12:34:33,497] Trial 4 finished with value: 2.039732553903569 and parameters: {'n_estimators': 829, 'learning_rate': 0.04779203540795714, 'max_depth': 6, 'min_child_weight': 0.062370183725939096, 'subsample': 0.9467914155859045, 'colsample_bytree': 0.742409473536463, 'gamma': 2.0605819778138654, 'reg_alpha': 8.476236353005083e-08, 'reg_lambda': 1.553363470872643e-07}. Best is trial 3 with value: 1.9794415155342833.\n",
      "[I 2025-10-25 12:34:33,892] Trial 5 pruned. \n",
      "[I 2025-10-25 12:34:34,879] Trial 6 finished with value: 1.9467379684716604 and parameters: {'n_estimators': 455, 'learning_rate': 0.03724918803767672, 'max_depth': 4, 'min_child_weight': 0.0187034013138803, 'subsample': 0.5439867266199441, 'colsample_bytree': 0.8079967431540918, 'gamma': 1.0842075132055673, 'reg_alpha': 3.5088483905980625e-07, 'reg_lambda': 0.00045680584016718784}. Best is trial 6 with value: 1.9467379684716604.\n",
      "[I 2025-10-25 12:34:35,386] Trial 7 pruned. \n",
      "[I 2025-10-25 12:34:35,666] Trial 8 pruned. \n",
      "[I 2025-10-25 12:34:36,038] Trial 9 pruned. \n",
      "[I 2025-10-25 12:34:37,258] Trial 10 finished with value: 1.8897709903539048 and parameters: {'n_estimators': 309, 'learning_rate': 0.01592352604835588, 'max_depth': 4, 'min_child_weight': 7.602428019320027, 'subsample': 0.5247324098918887, 'colsample_bytree': 0.8345548204228548, 'gamma': 0.03664472230088922, 'reg_alpha': 3.5580171106571696, 'reg_lambda': 3.770686264592387e-06}. Best is trial 10 with value: 1.8897709903539048.\n",
      "[I 2025-10-25 12:34:38,702] Trial 11 finished with value: 1.898030455831572 and parameters: {'n_estimators': 427, 'learning_rate': 0.01197400273802527, 'max_depth': 4, 'min_child_weight': 7.243853222250748, 'subsample': 0.5205672349442528, 'colsample_bytree': 0.8378304823180647, 'gamma': 0.11745064061365651, 'reg_alpha': 5.04723663018768, 'reg_lambda': 2.6794996538160077e-06}. Best is trial 10 with value: 1.8897709903539048.\n",
      "[I 2025-10-25 12:34:38,905] Trial 12 pruned. \n",
      "[I 2025-10-25 12:34:40,815] Trial 13 finished with value: 1.8674240203366295 and parameters: {'n_estimators': 693, 'learning_rate': 0.015461777770112319, 'max_depth': 9, 'min_child_weight': 8.108575679144865, 'subsample': 0.6093252292813798, 'colsample_bytree': 0.8645552203636571, 'gamma': 0.12039811534583056, 'reg_alpha': 4.733806245003293, 'reg_lambda': 2.9559030883973196e-06}. Best is trial 13 with value: 1.8674240203366295.\n",
      "[I 2025-10-25 12:34:44,986] Trial 14 finished with value: 1.8815170158253447 and parameters: {'n_estimators': 750, 'learning_rate': 0.0188340760252129, 'max_depth': 9, 'min_child_weight': 2.282871555425751, 'subsample': 0.6187777583695806, 'colsample_bytree': 0.9053466718470587, 'gamma': 0.8593902034965781, 'reg_alpha': 0.0952828454039955, 'reg_lambda': 2.970885623021025e-06}. Best is trial 13 with value: 1.8674240203366295.\n",
      "[I 2025-10-25 12:34:46,548] Trial 15 finished with value: 1.864204713716947 and parameters: {'n_estimators': 751, 'learning_rate': 0.020629406956438008, 'max_depth': 9, 'min_child_weight': 2.928338578784253, 'subsample': 0.634652702671491, 'colsample_bytree': 0.9335168792205203, 'gamma': 0.8199305686783146, 'reg_alpha': 0.06142071428261951, 'reg_lambda': 2.5112206398413934e-05}. Best is trial 15 with value: 1.864204713716947.\n",
      "[I 2025-10-25 12:34:49,161] Trial 16 finished with value: 1.8663874115601324 and parameters: {'n_estimators': 1277, 'learning_rate': 0.021317312705061823, 'max_depth': 9, 'min_child_weight': 2.293422824724815, 'subsample': 0.6324804773715551, 'colsample_bytree': 0.98432983243319, 'gamma': 0.9619926495475011, 'reg_alpha': 0.02097853487133794, 'reg_lambda': 0.0023493872159711473}. Best is trial 15 with value: 1.864204713716947.\n",
      "[I 2025-10-25 12:34:51,376] Trial 17 pruned. \n",
      "[I 2025-10-25 12:34:52,071] Trial 18 pruned. \n",
      "[I 2025-10-25 12:34:52,480] Trial 19 pruned. \n",
      "[I 2025-10-25 12:34:53,502] Trial 20 pruned. \n",
      "[I 2025-10-25 12:34:56,476] Trial 21 finished with value: 1.8513804356283017 and parameters: {'n_estimators': 738, 'learning_rate': 0.01381094099500157, 'max_depth': 9, 'min_child_weight': 3.9570189423427333, 'subsample': 0.5797180465561143, 'colsample_bytree': 0.8945263014797376, 'gamma': 0.5868244085239291, 'reg_alpha': 0.9994562399544696, 'reg_lambda': 4.249978433821128e-07}. Best is trial 21 with value: 1.8513804356283017.\n",
      "[I 2025-10-25 12:34:59,551] Trial 22 finished with value: 1.8456237163824807 and parameters: {'n_estimators': 1219, 'learning_rate': 0.012575381402791476, 'max_depth': 10, 'min_child_weight': 4.062027538647913, 'subsample': 0.5752308436212092, 'colsample_bytree': 0.8973443275496888, 'gamma': 0.6408096049003683, 'reg_alpha': 0.002591098838116064, 'reg_lambda': 2.8059477624753025e-07}. Best is trial 22 with value: 1.8456237163824807.\n",
      "[I 2025-10-25 12:35:01,678] Trial 23 finished with value: 1.823531499561576 and parameters: {'n_estimators': 944, 'learning_rate': 0.011747081858344678, 'max_depth': 10, 'min_child_weight': 3.42007073284716, 'subsample': 0.566314169929096, 'colsample_bytree': 0.7877135968699066, 'gamma': 0.5791142211686617, 'reg_alpha': 0.002147779931983191, 'reg_lambda': 2.3173364541158581e-07}. Best is trial 23 with value: 1.823531499561576.\n",
      "[I 2025-10-25 12:35:04,587] Trial 24 finished with value: 1.8398393707029448 and parameters: {'n_estimators': 1037, 'learning_rate': 0.012109121586949392, 'max_depth': 11, 'min_child_weight': 1.1843241097321362, 'subsample': 0.5685173909645685, 'colsample_bytree': 0.7956392502254006, 'gamma': 0.6042833875239431, 'reg_alpha': 0.001704384257354639, 'reg_lambda': 3.0772457018726963e-07}. Best is trial 23 with value: 1.823531499561576.\n",
      "[I 2025-10-25 12:35:05,130] Trial 25 pruned. \n",
      "[I 2025-10-25 12:35:07,272] Trial 26 finished with value: 1.8263385878384313 and parameters: {'n_estimators': 936, 'learning_rate': 0.012017781711500976, 'max_depth': 11, 'min_child_weight': 1.1457163691416747, 'subsample': 0.6811657754830577, 'colsample_bytree': 0.8802209721258656, 'gamma': 0.5527322371394177, 'reg_alpha': 0.0012616974876950085, 'reg_lambda': 3.636764634511925e-07}. Best is trial 23 with value: 1.823531499561576.\n",
      "[I 2025-10-25 12:35:09,417] Trial 27 finished with value: 1.8280685370719596 and parameters: {'n_estimators': 939, 'learning_rate': 0.012697822706397176, 'max_depth': 11, 'min_child_weight': 1.1725413587432871, 'subsample': 0.6821386401568102, 'colsample_bytree': 0.8138805951762219, 'gamma': 0.4600268213182389, 'reg_alpha': 7.955882875208541e-06, 'reg_lambda': 5.141635289349591e-07}. Best is trial 23 with value: 1.823531499561576.\n",
      "[I 2025-10-25 12:35:11,658] Trial 28 finished with value: 1.870448905643893 and parameters: {'n_estimators': 907, 'learning_rate': 0.06419440276576643, 'max_depth': 10, 'min_child_weight': 0.30467235953765304, 'subsample': 0.7010171783621596, 'colsample_bytree': 0.7691004527458547, 'gamma': 0.4213359970496797, 'reg_alpha': 2.3456798024547835e-06, 'reg_lambda': 7.851717565938272e-08}. Best is trial 23 with value: 1.823531499561576.\n",
      "[I 2025-10-25 12:35:11,955] Trial 29 pruned. \n",
      "[I 2025-10-25 12:35:12,306] Trial 30 pruned. \n",
      "[I 2025-10-25 12:35:14,701] Trial 31 finished with value: 1.810323059330426 and parameters: {'n_estimators': 1033, 'learning_rate': 0.012006708719552285, 'max_depth': 11, 'min_child_weight': 1.2151341041995083, 'subsample': 0.6830722384603569, 'colsample_bytree': 0.7952292119919881, 'gamma': 0.426196214874468, 'reg_alpha': 0.0006529992109183216, 'reg_lambda': 6.917600533322304e-07}. Best is trial 31 with value: 1.810323059330426.\n",
      "[I 2025-10-25 12:35:18,153] Trial 32 finished with value: 1.7972009496258785 and parameters: {'n_estimators': 867, 'learning_rate': 0.01025557781974342, 'max_depth': 10, 'min_child_weight': 1.1141452098165265, 'subsample': 0.682444539742094, 'colsample_bytree': 0.8153022194774273, 'gamma': 0.400828486905724, 'reg_alpha': 0.0007415513625505704, 'reg_lambda': 9.74453754947614e-07}. Best is trial 32 with value: 1.7972009496258785.\n",
      "[I 2025-10-25 12:35:18,505] Trial 33 pruned. \n",
      "[I 2025-10-25 12:35:20,719] Trial 34 finished with value: 1.800445850612629 and parameters: {'n_estimators': 866, 'learning_rate': 0.016772895620409546, 'max_depth': 12, 'min_child_weight': 0.17174296864980812, 'subsample': 0.786053313129889, 'colsample_bytree': 0.7697138257050298, 'gamma': 0.3233129109638552, 'reg_alpha': 0.0005580658726716817, 'reg_lambda': 8.838966322273638e-07}. Best is trial 32 with value: 1.7972009496258785.\n",
      "[I 2025-10-25 12:35:22,603] Trial 35 finished with value: 1.8598721510608935 and parameters: {'n_estimators': 849, 'learning_rate': 0.017954336961948987, 'max_depth': 12, 'min_child_weight': 0.19801591243872, 'subsample': 0.8340273826773708, 'colsample_bytree': 0.6556890593643937, 'gamma': 0.33599353877668325, 'reg_alpha': 0.00038099878525289424, 'reg_lambda': 3.2285322411250686e-08}. Best is trial 32 with value: 1.7972009496258785.\n",
      "[I 2025-10-25 12:35:22,940] Trial 36 pruned. \n",
      "[I 2025-10-25 12:35:23,208] Trial 37 pruned. \n",
      "[I 2025-10-25 12:35:23,654] Trial 38 pruned. \n",
      "[I 2025-10-25 12:35:26,774] Trial 39 finished with value: 1.814934088946996 and parameters: {'n_estimators': 1758, 'learning_rate': 0.04512533890527856, 'max_depth': 12, 'min_child_weight': 0.5268028988181885, 'subsample': 0.7052216872129662, 'colsample_bytree': 0.7730884330679484, 'gamma': 0.26097019122954923, 'reg_alpha': 0.00019206644469456398, 'reg_lambda': 0.0007030186354473608}. Best is trial 32 with value: 1.7972009496258785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sMAPE: 1.7972009496258785\n",
      "Best params: {'n_estimators': 867, 'learning_rate': 0.01025557781974342, 'max_depth': 10, 'min_child_weight': 1.1141452098165265, 'subsample': 0.682444539742094, 'colsample_bytree': 0.8153022194774273, 'gamma': 0.400828486905724, 'reg_alpha': 0.0007415513625505704, 'reg_lambda': 9.74453754947614e-07}\n",
      "\n",
      "=== STAPLES TEST METRICS ===\n",
      "MAE : 10.155\n",
      "RMSE: 23.676\n",
      "sMAPE: 6.57%\n",
      "\n",
      "[2025-10-25 12:35:27.607455] Building 6-month recursive forecasts…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:393: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby([\"admin_1\",\"product\"], as_index=False)\n",
      "/var/folders/z1/szmsj4mj2_jgj9dx7p3cz92w0000gn/T/ipykernel_29268/1019645448.py:251: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for (adm, prod), g in last_hist.groupby([\"admin_1\",\"product\"], sort=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved:\n",
      "  Model           -> /Users/nataschajademinnitt/Documents/5_data/food_security/etl/artifacts/xgb_staples_model.joblib\n",
      "  Feature list    -> /Users/nataschajademinnitt/Documents/5_data/food_security/etl/artifacts/xgb_staples_features.json\n",
      "  Last history    -> /Users/nataschajademinnitt/Documents/5_data/food_security/etl/artifacts/xgb_staples_last_history.parquet\n",
      "  Test predictions-> /Users/nataschajademinnitt/Documents/5_data/food_security/etl/artifacts/xgb_staples_test_predictions.parquet\n",
      "  Future forecasts-> /Users/nataschajademinnitt/Documents/5_data/food_security/etl/artifacts/xgb_staples_future_forecast.parquet\n"
     ]
    }
   ],
   "source": [
    "# ======================= train_and_export.py =======================\n",
    "# Trains the staples-only model, exports model + metadata,\n",
    "# and produces multi-month recursive forecasts for Streamlit.\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "PARQUET_PATH = \"/Users/nataschajademinnitt/Documents/5_data/food_security/etl/ethiopia_foodprices_model_panel.parquet\"\n",
    "OUTPUT_DIR    = Path(\"/Users/nataschajademinnitt/Documents/5_data/food_security/etl/artifacts\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEST_HORIZON_MONTHS = 6             # last H months = test\n",
    "OPTUNA_TRIALS       = 40\n",
    "FORECAST_HORIZON    = 6             # months to predict into the future (can increase)\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def month_start(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-9) -> float:\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return float(np.mean(np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "KEY_CATS = {\"admin_1\", \"product\"}\n",
    "NUMERIC_LIKELY = {\n",
    "    \"value_imputed\",\"value_mean\",\"value_median\",\"value_orig\",\"n_obs\",\n",
    "    \"rfh_month\",\"rfh_avg_month\",\"rfq_month\",\"rain_anom_pct\",\n",
    "    \"fao_category_index\",\"fao_food_price_index\",\n",
    "    \"ptm_severity\",\"population_2023\",\n",
    "    \"y\",\"y_lag1\",\"y_lag3\",\"y_lag6\",\"y_lag12\",\n",
    "    \"roll_mean_3\",\"roll_std_3\",\"roll_mean_12\",\n",
    "    \"month_num\",\"mo_sin\",\"mo_cos\",\n",
    "}\n",
    "\n",
    "def decategorize_numerics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if c in KEY_CATS:\n",
    "            if not pd.api.types.is_categorical_dtype(out[c]):\n",
    "                out[c] = out[c].astype(\"category\")\n",
    "            continue\n",
    "        if (c in NUMERIC_LIKELY or\n",
    "            pd.api.types.is_categorical_dtype(out[c]) or\n",
    "            pd.api.types.is_object_dtype(out[c])):\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols: list[str], dtype=\"float64\") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            out[c] = np.nan\n",
    "        if dtype is not None:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                try:\n",
    "                    out[c] = out[c].astype(dtype)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return out\n",
    "\n",
    "def fit_xgb_compat(params, X_tr, y_tr, X_val=None, y_val=None, early_rounds=100):\n",
    "    # Try newer API (>=2.0)\n",
    "    try:\n",
    "        from xgboost.callback import EarlyStopping\n",
    "        mdl = XGBRegressor(**params)\n",
    "        if X_val is not None:\n",
    "            mdl.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[EarlyStopping(rounds=early_rounds, save_best=True)])\n",
    "        else:\n",
    "            mdl.fit(X_tr, y_tr)\n",
    "        return mdl\n",
    "    except (TypeError, ImportError):\n",
    "        pass\n",
    "    # Fallback older\n",
    "    try:\n",
    "        mdl = XGBRegressor(**{k:v for k,v in params.items() if k!=\"eval_metric\"})\n",
    "        if X_val is not None:\n",
    "            mdl.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                    eval_metric=params.get(\"eval_metric\",\"rmse\"),\n",
    "                    early_stopping_rounds=early_rounds)\n",
    "        else:\n",
    "            mdl.fit(X_tr, y_tr)\n",
    "        return mdl\n",
    "    except TypeError:\n",
    "        pass\n",
    "    # Last\n",
    "    mdl = XGBRegressor(**{k:v for k,v in params.items() if k!=\"eval_metric\"})\n",
    "    mdl.fit(X_tr, y_tr)\n",
    "    return mdl\n",
    "\n",
    "def impute_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = decategorize_numerics(df_in)\n",
    "    df[\"month\"] = month_start(df[\"month\"])\n",
    "\n",
    "    rain_cols = [\"rfh_month\",\"rfh_avg_month\",\"rfq_month\",\"rain_anom_pct\"]\n",
    "    fao_cols  = [\"fao_category_index\",\"fao_food_price_index\"]\n",
    "    df = ensure_cols(df, rain_cols + fao_cols)\n",
    "\n",
    "    # flags\n",
    "    for c in rain_cols:\n",
    "        df[f\"{c}_was_missing\"] = df[c].isna().astype(\"int8\")\n",
    "\n",
    "    # recompute anomaly where possible\n",
    "    can = df[\"rfh_month\"].notna() & df[\"rfh_avg_month\"].gt(0)\n",
    "    df.loc[can,\"rain_anom_pct\"] = 100.0*(df.loc[can,\"rfh_month\"]/df.loc[can,\"rfh_avg_month\"] - 1.0)\n",
    "\n",
    "    # same-month median fill\n",
    "    for c in rain_cols + fao_cols:\n",
    "        df[c] = df[c].fillna(df.groupby(\"month\", observed=False)[c].transform(\"median\"))\n",
    "\n",
    "    # gentle past-only rolling fill by admin\n",
    "    df = df.sort_values([\"admin_1\",\"month\"])\n",
    "    for c in rain_cols:\n",
    "        df[c] = (df.groupby(\"admin_1\", group_keys=False)[c]\n",
    "                   .apply(lambda s: s.fillna(s.shift(1).rolling(3, min_periods=1).mean())))\n",
    "\n",
    "    # ptm & population\n",
    "    if \"ptm_severity\" in df.columns:\n",
    "        df[\"ptm_missing\"] = df[\"ptm_severity\"].isna().astype(\"int8\")\n",
    "        df[\"ptm_severity\"] = df[\"ptm_severity\"].fillna(0).astype(\"int8\")\n",
    "    else:\n",
    "        df[\"ptm_missing\"] = 1; df[\"ptm_severity\"] = 0\n",
    "\n",
    "    if \"population_2023\" in df.columns:\n",
    "        df[\"pop_missing\"] = df[\"population_2023\"].isna().astype(\"int8\")\n",
    "        adm_med = df.groupby(\"admin_1\", observed=False)[\"population_2023\"].transform(\"median\")\n",
    "        df[\"population_2023\"] = df[\"population_2023\"].fillna(adm_med)\n",
    "        df[\"population_2023\"] = df[\"population_2023\"].fillna(df[\"population_2023\"].median())\n",
    "    else:\n",
    "        df[\"pop_missing\"] = 1; df[\"population_2023\"] = np.nan\n",
    "\n",
    "    # FAO forward-fill by time\n",
    "    df = df.sort_values([\"month\"])\n",
    "    for c in fao_cols:\n",
    "        df[c] = df[c].ffill()\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.sort_values([\"admin_1\",\"product\",\"month\"]).copy()\n",
    "    df[\"y\"] = df[\"value_imputed\"].astype(\"float32\")\n",
    "\n",
    "    g = df.groupby([\"admin_1\",\"product\"], group_keys=False)[\"y\"]\n",
    "    df[\"y_lag1\"]  = g.shift(1)\n",
    "    df[\"y_lag3\"]  = g.shift(3)\n",
    "    df[\"y_lag6\"]  = g.shift(6)\n",
    "    df[\"y_lag12\"] = g.shift(12)\n",
    "\n",
    "    df[\"roll_mean_3\"]  = g.apply(lambda s: s.shift(1).rolling(3,  min_periods=1).mean())\n",
    "    df[\"roll_std_3\"]   = g.apply(lambda s: s.shift(1).rolling(3,  min_periods=2).std())\n",
    "    df[\"roll_mean_12\"] = g.apply(lambda s: s.shift(1).rolling(12, min_periods=3).mean())\n",
    "\n",
    "    df[\"month_num\"] = df[\"month\"].dt.month\n",
    "    df[\"mo_sin\"] = np.sin(2*np.pi*df[\"month_num\"]/12.0)\n",
    "    df[\"mo_cos\"] = np.cos(2*np.pi*df[\"month_num\"]/12.0)\n",
    "\n",
    "    # minimal history\n",
    "    df = df[df[\"y_lag1\"].notna()].copy()\n",
    "    return df\n",
    "\n",
    "def encode_ids(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in [\"admin_1\",\"product\"]:\n",
    "        out[c] = out[c].astype(\"category\")\n",
    "        out[f\"{c}_code\"] = out[c].cat.codes.astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def pick_features(df: pd.DataFrame) -> list[str]:\n",
    "    drop = {\n",
    "        \"y\",\"month\",\"admin_1\",\"product\",\n",
    "        \"value_imputed\",\"value_median\",\"value_mean\",\"value_orig\",\n",
    "        \"n_obs\",\"impute_method\",\"sources\",\"period_date\"\n",
    "    }\n",
    "    return [c for c in df.columns if c not in drop]\n",
    "\n",
    "def time_split(df: pd.DataFrame, horizon=6):\n",
    "    df = df.sort_values([\"month\",\"admin_1\",\"product\"]).reset_index(drop=True)\n",
    "    last_m = df[\"month\"].max().to_period(\"M\").to_timestamp()\n",
    "    cut_m  = (last_m.to_period(\"M\") - horizon).to_timestamp()\n",
    "    return df[df[\"month\"] <= cut_m].copy(), df[df[\"month\"] > cut_m].copy()\n",
    "\n",
    "def tune_xgb(X_train, y_train, n_trials=40, seed=42):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 2000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "            \"tree_method\": \"hist\", \"random_state\": seed, \"n_jobs\": -1, \"eval_metric\": \"rmse\",\n",
    "        }\n",
    "        smapes = []\n",
    "        for tr_idx, va_idx in tscv.split(X_train):\n",
    "            X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "            y_tr, y_val = y_train[tr_idx],    y_train[va_idx]\n",
    "            mdl = fit_xgb_compat(params, X_tr, y_tr, X_val, y_val, early_rounds=100)\n",
    "            smapes.append(smape(y_val, mdl.predict(X_val)))\n",
    "            trial.report(float(np.mean(smapes)), step=len(smapes))\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        return float(np.mean(smapes))\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=\"xgb-staples-smape\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    print(\"Best sMAPE:\", study.best_value)\n",
    "    print(\"Best params:\", study.best_trial.params)\n",
    "    best = study.best_trial.params\n",
    "    best.update({\"tree_method\":\"hist\",\"random_state\":seed,\"n_jobs\":-1,\"eval_metric\":\"rmse\"})\n",
    "    return best\n",
    "\n",
    "# -------------- Recursive forecast builder --------------\n",
    "def future_months(start_month: pd.Timestamp, horizon: int) -> pd.DatetimeIndex:\n",
    "    start_m = start_month.to_period(\"M\")\n",
    "    rng = pd.period_range(start=start_m + 1, periods=horizon, freq=\"M\").to_timestamp()\n",
    "    return pd.DatetimeIndex(rng)\n",
    "\n",
    "def recursive_forecast(model: XGBRegressor,\n",
    "                       last_hist: pd.DataFrame,\n",
    "                       feats: list[str],\n",
    "                       horizon: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    last_hist: DataFrame with columns ['admin_1','product','month','y', lag/roll features...]\n",
    "               containing at least the last 12 months per series to compute features.\n",
    "    Returns forecast rows for next `horizon` months with columns:\n",
    "      admin_1, product, month, y_pred\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    # Work per series to maintain its own lag/rolling state\n",
    "    for (adm, prod), g in last_hist.groupby([\"admin_1\",\"product\"], sort=False):\n",
    "        g = g.sort_values(\"month\").copy()\n",
    "        # Use the last available month to start\n",
    "        last_m = g[\"month\"].max()\n",
    "        future_idx = future_months(last_m, horizon)\n",
    "        # We’ll maintain a small working frame with just needed cols\n",
    "        work = g[[\"month\",\"y\"]].copy()\n",
    "        # Precompute month_num→sin/cos for all future months\n",
    "        mo_num = future_idx.month\n",
    "        mo_sin = np.sin(2*np.pi*mo_num/12.0)\n",
    "        mo_cos = np.cos(2*np.pi*mo_num/12.0)\n",
    "\n",
    "        # Values for exogenous features in the future:\n",
    "        # simple, robust choice = keep last known per series (you can replace with true future drivers if available)\n",
    "        exo_cols = [c for c in feats if c not in {\"admin_1_code\",\"product_code\",\n",
    "                                                  \"y_lag1\",\"y_lag3\",\"y_lag6\",\"y_lag12\",\n",
    "                                                  \"roll_mean_3\",\"roll_std_3\",\"roll_mean_12\",\n",
    "                                                  \"month_num\",\"mo_sin\",\"mo_cos\"}]\n",
    "        exo_defaults = {}\n",
    "        if exo_cols:\n",
    "            last_exo = g.iloc[-1:].reindex(columns=exo_cols).to_dict(orient=\"records\")[0]\n",
    "            exo_defaults = {k: (0.0 if pd.isna(v) else float(v)) for k, v in last_exo.items()}\n",
    "\n",
    "        # Codes (fixed)\n",
    "        admin_code = int(g[\"admin_1_code\"].iloc[-1]) if \"admin_1_code\" in g.columns else 0\n",
    "        product_code = int(g[\"product_code\"].iloc[-1]) if \"product_code\" in g.columns else 0\n",
    "\n",
    "        # Iterate months\n",
    "        for i, (m, s, c) in enumerate(zip(future_idx, mo_sin, mo_cos)):\n",
    "            # construct feature row using *past-only* info\n",
    "            # compute lags from `work`\n",
    "            y_lag1  = work[\"y\"].iloc[-1] if len(work) >= 1 else np.nan\n",
    "            y_lag3  = work[\"y\"].iloc[-3] if len(work) >= 3 else np.nan\n",
    "            y_lag6  = work[\"y\"].iloc[-6] if len(work) >= 6 else np.nan\n",
    "            y_lag12 = work[\"y\"].iloc[-12] if len(work) >= 12 else np.nan\n",
    "\n",
    "            roll_mean_3  = work[\"y\"].shift(1).rolling(3,  min_periods=1).mean().iloc[-1] if len(work) >= 1 else np.nan\n",
    "            roll_std_3   = work[\"y\"].shift(1).rolling(3,  min_periods=2).std().iloc[-1]  if len(work) >= 2 else np.nan\n",
    "            roll_mean_12 = work[\"y\"].shift(1).rolling(12, min_periods=3).mean().iloc[-1] if len(work) >= 3 else np.nan\n",
    "\n",
    "            month_num = m.month\n",
    "\n",
    "            feat_row = {\n",
    "                \"admin_1_code\": admin_code,\n",
    "                \"product_code\": product_code,\n",
    "                \"y_lag1\": y_lag1, \"y_lag3\": y_lag3, \"y_lag6\": y_lag6, \"y_lag12\": y_lag12,\n",
    "                \"roll_mean_3\": roll_mean_3, \"roll_std_3\": roll_std_3, \"roll_mean_12\": roll_mean_12,\n",
    "                \"month_num\": month_num, \"mo_sin\": s, \"mo_cos\": c,\n",
    "            }\n",
    "            # add exogenous defaults\n",
    "            feat_row.update(exo_defaults)\n",
    "\n",
    "            # Align to model features (missing → NaN -> model can handle)\n",
    "            X_f = pd.DataFrame([feat_row])[feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "            # Predict (remember model was trained on log1p(y))\n",
    "            y_hat = float(np.expm1(model.predict(X_f)[0]))\n",
    "            out_rows.append({\"admin_1\": adm, \"product\": prod, \"month\": m, \"y_pred\": y_hat})\n",
    "\n",
    "            # append to work to update lags for next step\n",
    "            work = pd.concat([work, pd.DataFrame({\"month\":[m], \"y\":[y_hat]})], ignore_index=True)\n",
    "\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "# ============================== TRAIN & EXPORT ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load\n",
    "    panel = pd.read_parquet(PARQUET_PATH)\n",
    "    assert {\"admin_1\",\"product\",\"month\",\"value_imputed\"}.issubset(panel.columns)\n",
    "\n",
    "    # de-categorize once\n",
    "    panel = decategorize_numerics(panel)\n",
    "    # force any non-key categoricals to numeric\n",
    "    for c in panel.columns:\n",
    "        if pd.api.types.is_categorical_dtype(panel[c]) and c not in KEY_CATS:\n",
    "            panel[c] = pd.to_numeric(panel[c].astype(object), errors=\"coerce\")\n",
    "\n",
    "    # 2) Drop livestock\n",
    "    LIVESTOCK = {\n",
    "        \"Goats (Local Quality)\",\"Sheep (Local Quality)\",\n",
    "        \"Camels (Local Quality)\",\"Oxen (Local Quality)\"\n",
    "    }\n",
    "    staples = panel[~panel[\"product\"].isin(LIVESTOCK)].copy()\n",
    "\n",
    "    # 3) Features\n",
    "    staples = impute_features(staples)\n",
    "    df_feat  = build_features(staples)\n",
    "    df_feat  = encode_ids(df_feat)\n",
    "    df_feat  = df_feat.sort_values([\"month\",\"admin_1\",\"product\"]).reset_index(drop=True)\n",
    "    feats = [c for c in df_feat.columns if c not in {\n",
    "        \"y\",\"month\",\"admin_1\",\"product\",\n",
    "        \"value_imputed\",\"value_median\",\"value_mean\",\"value_orig\",\n",
    "        \"n_obs\",\"impute_method\",\"sources\",\"period_date\"\n",
    "    }]\n",
    "    df_feat[feats] = df_feat[feats].apply(pd.to_numeric, errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    # 4) Split\n",
    "    train_df = df_feat[df_feat[\"month\"] <= (df_feat[\"month\"].max().to_period(\"M\") - TEST_HORIZON_MONTHS).to_timestamp()].copy()\n",
    "    test_df  = df_feat[df_feat[\"month\"]  > (df_feat[\"month\"].max().to_period(\"M\") - TEST_HORIZON_MONTHS).to_timestamp()].copy()\n",
    "    X_train, X_test = train_df[feats], test_df[feats]\n",
    "    y_train_log = np.log1p(train_df[\"y\"].to_numpy())\n",
    "    y_test_true = test_df[\"y\"].to_numpy()\n",
    "\n",
    "    # 5) Tune\n",
    "    print(f\"[{datetime.now()}] Tuning ({OPTUNA_TRIALS} trials)…\")\n",
    "    best_params = tune_xgb(X_train, y_train_log, n_trials=OPTUNA_TRIALS, seed=42)\n",
    "\n",
    "    # 6) Early stopping on tail of TRAIN\n",
    "    last_train_m = train_df[\"month\"].max()\n",
    "    val_cut = (last_train_m.to_period(\"M\") - 2).to_timestamp()  # last 3 months\n",
    "    val_mask = train_df[\"month\"] >= val_cut\n",
    "    X_tr_es, y_tr_es = X_train[~val_mask], y_train_log[~val_mask]\n",
    "    X_va_es, y_va_es = X_train[val_mask],  y_train_log[val_mask]\n",
    "    if len(X_va_es) == 0 or len(X_tr_es) == 0:\n",
    "        X_tr_es, y_tr_es = X_train, y_train_log\n",
    "        X_va_es = y_va_es = None\n",
    "\n",
    "    # 7) Fit final & predict test\n",
    "    final_model = fit_xgb_compat(best_params, X_tr_es, y_tr_es, X_va_es, y_va_es, early_rounds=200)\n",
    "    y_pred_test = np.expm1(final_model.predict(X_test))\n",
    "\n",
    "    print(\"\\n=== STAPLES TEST METRICS ===\")\n",
    "    print(f\"MAE : {np.mean(np.abs(y_test_true - y_pred_test)):,.3f}\")\n",
    "    print(f\"RMSE: {rmse(y_test_true, y_pred_test):,.3f}\")\n",
    "    print(f\"sMAPE: {smape(y_test_true, y_pred_test):.2f}%\")\n",
    "\n",
    "    # 8) Save artifacts\n",
    "    model_path   = OUTPUT_DIR / \"xgb_staples_model.joblib\"\n",
    "    feats_path   = OUTPUT_DIR / \"xgb_staples_features.json\"\n",
    "    last_hist_pq = OUTPUT_DIR / \"xgb_staples_last_history.parquet\"\n",
    "    test_pred_pq = OUTPUT_DIR / \"xgb_staples_test_predictions.parquet\"\n",
    "\n",
    "    joblib.dump(final_model, model_path)\n",
    "    pd.Series(feats, name=\"features\").to_json(feats_path, orient=\"values\")\n",
    "\n",
    "    test_out = test_df[[\"admin_1\",\"product\",\"month\",\"y\"]].copy()\n",
    "    test_out[\"y_pred\"] = y_pred_test\n",
    "    test_out.to_parquet(test_pred_pq, index=False)\n",
    "\n",
    "    # 9) Build a compact \"last history\" frame for recursive forecasting\n",
    "    # Keep last 15 months per series to compute lags/rolls safely\n",
    "    last_blocks = (\n",
    "        df_feat.sort_values([\"admin_1\",\"product\",\"month\"])\n",
    "              .groupby([\"admin_1\",\"product\"], as_index=False)\n",
    "              .tail(15)\n",
    "              .reset_index(drop=True)\n",
    "    )\n",
    "    last_blocks.to_parquet(last_hist_pq, index=False)\n",
    "\n",
    "    # 10) Generate forward forecasts now (optional: or do it in Streamlit)\n",
    "    print(f\"\\n[{datetime.now()}] Building {FORECAST_HORIZON}-month recursive forecasts…\")\n",
    "    # Use the same features list `feats`\n",
    "    future_df = recursive_forecast(final_model, last_blocks, feats, horizon=FORECAST_HORIZON)\n",
    "    future_path = OUTPUT_DIR / \"xgb_staples_future_forecast.parquet\"\n",
    "    future_df.to_parquet(future_path, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\"  Model           ->\", model_path)\n",
    "    print(\"  Feature list    ->\", feats_path)\n",
    "    print(\"  Last history    ->\", last_hist_pq)\n",
    "    print(\"  Test predictions->\", test_pred_pq)\n",
    "    print(\"  Future forecasts->\", future_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
